{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/mal_adv3/blob/main/malware_cnn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming binary_vector is your 1D binary vector of length 10000\n",
        "binary_vector = np.random.randint(2, size=9800)\n",
        "\n",
        "# Reshape the vector into a 2D array with each element being an 8-length binary vector\n",
        "image = np.reshape(binary_vector, (35, 35, 8))\n",
        "\n",
        "# Convert binary values to gray-scale intensities\n",
        "gray_image = np.packbits(image, axis=-1)\n",
        "\n",
        "# Display the gray-scale image\n",
        "plt.imshow(gray_image, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "4ZG8enqYt5ga",
        "outputId": "cad3e974-7ddb-4542-809d-d1fe09b8baa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZfX_N-gQHKp",
        "outputId": "82d6a8b9-0ab9-4ef3-8636-dd8466d29af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/X_redefined_sparse_matrix.npz\n",
            "To: /content/X_redefined_sparse_matrix.npz\n",
            "100%|██████████| 2.31M/2.31M [00:00<00:00, 13.9MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/labels.pt\n",
            "To: /content/labels.pt\n",
            "100%|██████████| 517k/517k [00:00<00:00, 6.80MB/s]\n"
          ]
        }
      ],
      "source": [
        "download_links = ['https://github.com/mostafa-ja/mal_adv3/raw/main/data/X_redefined_sparse_matrix.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/data/labels.pt'\n",
        "]\n",
        "\n",
        "\n",
        "import gdown\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usjRDnBCQK9l",
        "outputId": "2d95236b-4ebc-45cc-96e2-1dc06fffccbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "X_redefined = sparse.load_npz(\"X_redefined_sparse_matrix.npz\")\n",
        "labels_tensor = torch.load('labels.pt')\n",
        "\n",
        "# Split data into train, validation, and test sets with stratified sampling\n",
        "X_train_val, X_test, labels_train_val, labels_test = train_test_split(X_redefined, labels_tensor, test_size=0.2, stratify=labels_tensor, random_state=42)\n",
        "X_train, X_val, labels_train, labels_val = train_test_split(X_train_val, labels_train_val, test_size=0.2, stratify=labels_train_val, random_state=42)\n",
        "\n",
        "# Combine features and labels into datasets\n",
        "# we use dtype=torch.int8, for Memory-Efficient here, later we will convert to float\n",
        "train_dataset = TensorDataset(torch.tensor(X_train.toarray(), dtype=torch.int8), labels_train)\n",
        "val_dataset = TensorDataset(torch.tensor(X_val.toarray(), dtype=torch.int8), labels_val)\n",
        "test_dataset = TensorDataset(torch.tensor(X_test.toarray(), dtype=torch.int8), labels_test)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del X_redefined, labels_tensor, X_train_val, X_test, labels_train_val, labels_test, X_train, X_val, labels_train, labels_val"
      ],
      "metadata": {
        "id": "OlfUNSCJQQiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "_DeOfARzQRc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_binary_vector(binary_vector, new_size):\n",
        "    batch_size, vector_length = binary_vector.size()\n",
        "\n",
        "    # Calculate the number of zeros to add\n",
        "    num_zeros = new_size - vector_length\n",
        "\n",
        "    # Create a tensor filled with zeros\n",
        "    zeros_tensor = torch.zeros(batch_size, num_zeros, dtype=torch.int)\n",
        "\n",
        "    # Concatenate the binary vector with the zeros tensor along the second dimension\n",
        "    padded_binary_vector = torch.cat([binary_vector, zeros_tensor], dim=1)\n",
        "\n",
        "    return padded_binary_vector"
      ],
      "metadata": {
        "id": "U57qM9nEH6ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_binary_vectors_to_gray_images(binary_vectors, image_size=36):\n",
        "    \"\"\"\n",
        "    Convert a batch of binary vectors to gray-scale images.\n",
        "\n",
        "    Parameters:\n",
        "        batch_binary_vectors (numpy.ndarray): Batch of binary vectors with shape (batch_size, vector_length).\n",
        "        image_size (int): Size of the square image (default is 36).\n",
        "\n",
        "    Returns:\n",
        "        gray_images: Batch of gray-scale images with shape (batch_size, image_size, image_size).\n",
        "    \"\"\"\n",
        "    batch_binary_vectors = pad_binary_vector(binary_vectors, 10368)\n",
        "\n",
        "    # Reshape each binary vector in the batch into a 2D array with each element being an 8-length binary vector\n",
        "    batch_images = np.reshape(batch_binary_vectors, (-1, image_size, image_size, 8))\n",
        "\n",
        "    # Convert binary values to gray-scale intensities\n",
        "    gray_images = np.packbits(batch_images, axis=-1)\n",
        "\n",
        "    # Remove the last dimension (single channel) from the gray_images array\n",
        "    gray_images = np.squeeze(gray_images, axis=-1)\n",
        "\n",
        "    return gray_images\n"
      ],
      "metadata": {
        "id": "yo0LOBhSIDd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X,Y in train_loader:\n",
        "  print(X.shape)\n",
        "  print(Y.shape)\n",
        "  break\n",
        "\n",
        "m = batch_binary_vectors_to_gray_images(X, image_size=36)\n",
        "print(m.shape)\n",
        "\n",
        "plt.imshow(m[2], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "ev4l8hNVIMPA",
        "outputId": "ebbbbabf-cab0-4f59-e3c0-8fafee26bd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 10000])\n",
            "torch.Size([256, 1])\n",
            "(256, 36, 36)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.zeros(256, 10368, dtype=torch.int)\n",
        "a[:,:10000] =  X\n",
        "\n",
        "m = batch_binary_vectors_to_gray_images(a, image_size=36)\n",
        "print(m.shape)\n",
        "\n",
        "plt.imshow(m[2], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "NvbXIKk5I4_7",
        "outputId": "3e901740-56f5-43e1-b1e6-23240bfb741a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 36, 36)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # Change input channels from 1 to 2\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 9 * 9, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "FQ1ZC4oXI7ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define a learning rate warm-up and decay function\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < warmup_epochs:\n",
        "        return float(epoch+1) / float(max(1, warmup_epochs))\n",
        "    else:\n",
        "        return lr_decay ** (epoch - warmup_epochs)\n",
        "\n",
        "# Define warm-up epochs, learning rate decay factor, and other hyperparameters\n",
        "warmup_epochs = 5\n",
        "lr_decay = 0.97\n",
        "initial_lr = 0.001\n",
        "\n",
        "# Create an instance of the model and move it to the GPU\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "\n",
        "# Create a learning rate scheduler\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "# Initialize variables to track best F1 score and corresponding model state\n",
        "best_f1_score = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for X_train, labels_train in train_loader:\n",
        "        labels_train = labels_train.to(device)\n",
        "        images_train = torch.tensor(batch_binary_vectors_to_gray_images(X_train, image_size=36), device=device) / 255.0\n",
        "        optimizer.zero_grad()\n",
        "        outputs_train = model(images_train)\n",
        "        loss_train = criterion(outputs_train.squeeze(), labels_train.squeeze())\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_train.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        predicted_train = torch.round(outputs_train.squeeze())\n",
        "        total_train += labels_train.size(0)\n",
        "        correct_train += (predicted_train == labels_train.squeeze()).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print('-----------------------------------------------------')\n",
        "    print('Learning raet : ',optimizer.param_groups[0]['lr'])\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    model.eval()\n",
        "    true_labels_val = []\n",
        "    predicted_labels_val = []\n",
        "    with torch.no_grad():\n",
        "        for X_val, labels_val in val_loader:\n",
        "            labels_val = labels_val.to(device)\n",
        "            val_images = torch.tensor(batch_binary_vectors_to_gray_images(X_val, image_size=36), device=device) / 255.0\n",
        "            val_outputs = model(val_images)\n",
        "            predicted_val = torch.round(val_outputs.squeeze())\n",
        "\n",
        "            # Append true and predicted labels to the lists\n",
        "            true_labels_val.extend(labels_val.squeeze().tolist())\n",
        "            predicted_labels_val.extend(predicted_val.tolist())\n",
        "\n",
        "    # Convert the lists to numpy arrays\n",
        "    true_labels_val = np.array(true_labels_val)\n",
        "    predicted_labels_val = np.array(predicted_labels_val)\n",
        "\n",
        "    # Compute precision, recall, and F1 score for validation set\n",
        "    precision_val = precision_score(true_labels_val, predicted_labels_val)\n",
        "    recall_val = recall_score(true_labels_val, predicted_labels_val)\n",
        "    f1_val = f1_score(true_labels_val, predicted_labels_val)\n",
        "\n",
        "    print(f\"Validation Precision: {precision_val:.4f}, Recall: {recall_val:.4f}, F1 Score: {f1_val:.4f}\")\n",
        "\n",
        "    # Evaluate the model on the test dataset\n",
        "    true_labels_test = []\n",
        "    predicted_labels_test = []\n",
        "    with torch.no_grad():\n",
        "        for X_test, labels_test in test_loader:\n",
        "            labels_test = labels_test.to(device)\n",
        "            test_images = torch.tensor(batch_binary_vectors_to_gray_images(X_test, image_size=36), device=device) / 255.0\n",
        "            test_outputs = model(test_images)\n",
        "            predicted_test = torch.round(test_outputs.squeeze())\n",
        "\n",
        "            # Append true and predicted labels to the lists\n",
        "            true_labels_test.extend(labels_test.squeeze().tolist())\n",
        "            predicted_labels_test.extend(predicted_test.tolist())\n",
        "\n",
        "    # Convert the lists to numpy arrays\n",
        "    true_labels_test = np.array(true_labels_test)\n",
        "    predicted_labels_test = np.array(predicted_labels_test)\n",
        "\n",
        "    # Compute precision, recall, and F1 score for test set\n",
        "    precision_test = precision_score(true_labels_test, predicted_labels_test)\n",
        "    recall_test = recall_score(true_labels_test, predicted_labels_test)\n",
        "    f1_test = f1_score(true_labels_test, predicted_labels_test)\n",
        "\n",
        "    print(f\"Test Precision: {precision_test:.4f}, Recall: {recall_test:.4f}, F1 Score: {f1_test:.4f}\")\n",
        "\n",
        "    # Check if the current model has the best F1 score\n",
        "    if f1_val > best_f1_score:\n",
        "        best_f1_score = f1_val\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "# Save the best model based on F1\n"
      ],
      "metadata": {
        "id": "tZWCF0ddP3VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "best_model_state = torch.load('best_model.pth')\n",
        "model = CNN()\n",
        "model.load_state_dict(best_model_state)\n",
        "model.eval()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize test metrics\n",
        "true_labels_test = []\n",
        "predicted_labels_test = []\n",
        "\n",
        "# Perform testing\n",
        "with torch.no_grad():\n",
        "    for X_test, labels_test in test_loader:\n",
        "        labels_test = labels_test.to(device)\n",
        "        test_images = torch.tensor(batch_binary_vectors_to_gray_images(X_test, image_size=36),device=device) / 255.0\n",
        "        test_outputs = model(test_images)\n",
        "        predicted_test = torch.round(test_outputs.squeeze())\n",
        "\n",
        "        # Append true and predicted labels to the lists\n",
        "        true_labels_test.extend(labels_test.squeeze().tolist())\n",
        "        predicted_labels_test.extend(predicted_test.tolist())\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "true_labels_test = np.array(true_labels_test)\n",
        "predicted_labels_test = np.array(predicted_labels_test)\n",
        "\n",
        "# Compute precision, recall, and F1 score for test set\n",
        "precision_test = precision_score(true_labels_test, predicted_labels_test)\n",
        "recall_test = recall_score(true_labels_test, predicted_labels_test)\n",
        "f1_test = f1_score(true_labels_test, predicted_labels_test)\n",
        "\n",
        "print(f\"Test Precision: {precision_test:.4f}, Recall: {recall_test:.4f}, F1 Score: {f1_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsziOGQE3K-k",
        "outputId": "e55f56e1-3175-4508-a42f-e039907ca77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 0.9521, Recall: 0.8408, F1 Score: 0.8930\n"
          ]
        }
      ]
    }
  ]
}