{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/mal_adv3/blob/main/4_adverserial_attacks_ipynb_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_links = ['https://github.com/mostafa-ja/mal_adv3/raw/main/data/X_redefined_sparse_matrix.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/data/DNN_params%20.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/data/labels.pt',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/data/vocab.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/data/adverserial_attacks_functions.py',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/data/best_model%20_RFGSM.pth',\n",
        "]"
      ],
      "metadata": {
        "id": "NgkPo5lth4ru"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kzSbjaXGVeG",
        "outputId": "fa0fd371-7588-482c-8926-f0fef577fece"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/X_redefined_sparse_matrix.npz\n",
            "To: /content/X_redefined_sparse_matrix.npz\n",
            "100%|██████████| 2.31M/2.31M [00:00<00:00, 9.41MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/DNN_params%20.pth\n",
            "To: /content/DNN_params%20.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 60.1MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/labels.pt\n",
            "To: /content/labels.pt\n",
            "100%|██████████| 517k/517k [00:00<00:00, 6.12MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/vocab.pkl\n",
            "To: /content/vocab.pkl\n",
            "100%|██████████| 9.18M/9.18M [00:00<00:00, 17.5MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/adverserial_attacks_functions.py\n",
            "To: /content/adverserial_attacks_functions.py\n",
            "45.5kB [00:00, 50.9MB/s]                   \n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/data/best_model%20_RFGSM.pth\n",
            "To: /content/best_model%20_RFGSM.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 56.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import time\n",
        "\n",
        "from adverserial_attacks_functions import *\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "JKDdI3K9LrlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908b6564-75c3-4a28-b81d-eee8ee2e6cbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dictionary from the file\n",
        "with open('vocab.pkl', 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "for i, (key, value) in enumerate(vocab.items()):\n",
        "    print((key, value))\n",
        "    if i >= 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "L0rDH5Q6HOmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecaded00-4ecf-4d72-ae81-6b70ba39a167"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('android/media/mediaplayer->start', 141045)\n",
            "('android/app/activity->setcontentview', 140900)\n",
            "('android/os/vibrator->cancel', 141093)\n",
            "('android.permission.vibrate', 140720)\n",
            "('android.hardware.touchscreen', 137091)\n",
            "('android.intent.action.main', 138335)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory-Efficient but slow when we want to convert back to tensor by\n",
        ".to_dense().to(torch.float32)\n",
        "\n",
        "```\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X_redefined = sparse.load_npz(\"X_redefined_sparse_matrix.npz\")\n",
        "labels_tensor = torch.load('labels.pt')\n",
        "\n",
        "# Split data into train, validation, and test sets with stratified sampling\n",
        "X_train_val, X_test, labels_train_val, labels_test = train_test_split(X_redefined, labels_tensor, test_size=0.2, stratify=labels_tensor, random_state=42)\n",
        "X_train, X_val, labels_train, labels_val = train_test_split(X_train_val, labels_train_val, test_size=0.2, stratify=labels_train_val, random_state=42)\n",
        "\n",
        "# Create PyTorch sparse tensors directly from the sparse matrices\n",
        "train_dataset = TensorDataset(torch.sparse_coo_tensor(torch.tensor(X_train.nonzero()), torch.tensor(X_train.data), X_train.shape), labels_train)\n",
        "val_dataset = TensorDataset(torch.sparse_coo_tensor(torch.tensor(X_val.nonzero()), torch.tensor(X_val.data), X_val.shape), labels_val)\n",
        "test_dataset = TensorDataset(torch.sparse_coo_tensor(torch.tensor(X_test.nonzero()), torch.tensor(X_test.data), X_test.shape), labels_test)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del X_redefined, labels_tensor, X_train_val, X_test, labels_train_val, labels_test, X_train, X_val, labels_train, labels_val\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MhY3eqbYzWAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "X_redefined = sparse.load_npz(\"X_redefined_sparse_matrix.npz\")\n",
        "labels_tensor = torch.load('labels.pt')\n",
        "\n",
        "# Split data into train, validation, and test sets with stratified sampling\n",
        "X_train_val, X_test, labels_train_val, labels_test = train_test_split(X_redefined, labels_tensor, test_size=0.2, stratify=labels_tensor, random_state=42)\n",
        "X_train, X_val, labels_train, labels_val = train_test_split(X_train_val, labels_train_val, test_size=0.2, stratify=labels_train_val, random_state=42)\n",
        "\n",
        "# Combine features and labels into datasets\n",
        "# we use dtype=torch.int8, for Memory-Efficient here, later we will convert to float\n",
        "train_dataset = TensorDataset(torch.tensor(X_train.toarray(), dtype=torch.int8), labels_train)\n",
        "val_dataset = TensorDataset(torch.tensor(X_val.toarray(), dtype=torch.int8), labels_val)\n",
        "test_dataset = TensorDataset(torch.tensor(X_test.toarray(), dtype=torch.int8), labels_test)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del X_redefined, labels_tensor, X_train_val, X_test, labels_train_val, labels_test, X_train, X_val, labels_train, labels_val"
      ],
      "metadata": {
        "id": "AWfz_HxX0rYj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "T97WdRj9s2hk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MalwareDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size=10000, hidden_1_size=200, hidden_2_size=200, num_labels=2, dropout_prob=0.6):\n",
        "        super(MalwareDetectionModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_1_size = hidden_1_size\n",
        "        self.hidden_2_size = hidden_2_size\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_1_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_1_size, hidden_2_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_2_size, num_labels)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.log_softmax(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xdNbTvxTTqyw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_DNN = MalwareDetectionModel().to(device)\n",
        "# Load model parameters\n",
        "model_DNN.load_state_dict(torch.load('DNN_params%20.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "0MavlKAt6mb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8c7ff3-e6bf-4a1c-bbc1-6517f4c97e8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_training(model, train_loader, val_loader, attack, adv_epochs=10, lr=0.001, weight_decay=0., device=device, verbose=True, **kwargs):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_time = 0.\n",
        "    nbatches = len(train_loader)\n",
        "    best_acc_val = 0.\n",
        "    acc_val_adv_be = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(adv_epochs):\n",
        "        epoch_losses = []\n",
        "        epoch_accuracies = []\n",
        "\n",
        "        for idx_batch, (x_batch, y_batch) in enumerate(train_loader):\n",
        "            x_batch, y_batch = x_batch.to(torch.float32).to(device), y_batch.to(device)\n",
        "            batch_size = x_batch.shape[0]\n",
        "\n",
        "            # Separate malicious and benign samples\n",
        "            mal_x_batch, ben_x_batch = x_batch[y_batch.squeeze() == 1], x_batch[y_batch.squeeze() == 0]\n",
        "            mal_y_batch, ben_y_batch = y_batch[y_batch.squeeze() == 1], y_batch[y_batch.squeeze() == 0]\n",
        "\n",
        "            # Generate adversarial examples\n",
        "            model.eval()\n",
        "            pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "            x_batch = torch.cat([ben_x_batch, pertb_mal_x], dim=0)\n",
        "            y_batch = torch.cat([ben_y_batch, mal_y_batch])\n",
        "            model.train()\n",
        "\n",
        "            # Forward pass and backward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss_train = criterion(outputs, y_batch.view(-1).long())\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate metrics\n",
        "            epoch_losses.append(loss_train.item())\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            acc_train = (predicted == y_batch).sum().item() / len(y_batch)\n",
        "            epoch_accuracies.append(acc_train)\n",
        "\n",
        "            # Print batch level information\n",
        "            if verbose:\n",
        "                print(f'Mini batch: {idx_batch + 1}/{nbatches} | Epoch: {epoch + 1}/{adv_epochs} | Batch Loss: {loss_train.item():.4f} | Batch Accuracy: {acc_train * 100:.2f}%')\n",
        "\n",
        "        # Calculate epoch level metrics\n",
        "        mean_loss = np.mean(epoch_losses)\n",
        "        mean_accuracy = np.mean(epoch_accuracies) * 100\n",
        "\n",
        "        # Print epoch level information\n",
        "        if verbose:\n",
        "            print(f'Training loss (epoch level): {mean_loss:.4f} | Train accuracy: {mean_accuracy:.2f}%')\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        avg_acc_ad_val = []\n",
        "        avg_acc_val = []\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "            outputs = model(x_val)\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            acc_val = (predicted == y_val).sum().item() / len(y_val)\n",
        "            avg_acc_val.append(acc_val)\n",
        "\n",
        "            # Generate adversarial examples for validation set\n",
        "            mal_x_batch, mal_y_batch = x_val[y_val.squeeze() == 1], y_val[y_val.squeeze() == 1]\n",
        "            pertb_mal_x = attack(mal_x_batch, mal_y_batch, model)\n",
        "            outputs = model(pertb_mal_x)\n",
        "            _, y_pred = torch.topk(outputs, k=1)\n",
        "\n",
        "            acc_ad_val = (y_pred == 1.).sum().item() / len(y_pred)\n",
        "            avg_acc_ad_val.append(acc_ad_val)\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        assert len(avg_acc_ad_val) > 0\n",
        "        acc_all = (np.mean(avg_acc_val) + np.mean(avg_acc_ad_val)) / 2.\n",
        "\n",
        "        # Update best validation accuracy\n",
        "        if acc_all >= best_acc_val:\n",
        "            best_acc_val = acc_all\n",
        "            acc_val_adv_be = np.mean(avg_acc_ad_val)\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        # Print validation results\n",
        "        if verbose:\n",
        "            print(f\"\\tVal accuracy(without attack) {np.mean(avg_acc_val) * 100:.4}% and accuracy(with attack) {np.mean(avg_acc_ad_val) * 100:.4}% under attack and overall accuracy {acc_all * 100:.4}%.\")\n",
        "            print(f\"\\tModel select at epoch {best_epoch} with validation accuracy {best_acc_val * 100:.4}% and accuracy {acc_val_adv_be * 100:.4}% under attack.\")\n"
      ],
      "metadata": {
        "id": "B0v7V8W7lidM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AT-rFGSM: Adversarial Taraining based on rFGSM attack\n",
        "#model_AT = MalwareDetectionModel().to(device)\n",
        "\n",
        "#attack_param = {\"k\":50, \"epsilon\":0.02, 'random':True, \"is_sample\":False, 'is_report_loss_diff':True}\n",
        "#adversarial_training(model_AD, train_loader, val_loader, adv_epochs=50, attack=dfgsm_k, **attack_param)"
      ],
      "metadata": {
        "id": "E8zaItRUryCv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJ4_vnhJs22-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if attack == mimicry:\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "      del benign_samples\n",
        "\n",
        "    model.eval()\n",
        "    avg_acc_ad_test = []\n",
        "    avg_acc_test = []\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            outputs = model(x_test)\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            avg_acc_test.append(acc_test)\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(ben_x, mal_x_batch, model, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            _, y_pred = torch.topk(outputs, k=1)\n",
        "\n",
        "            acc_ad_test = (y_pred == 1.).sum().item() / len(y_pred)\n",
        "            avg_acc_ad_test.append(acc_ad_test)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Adversarial accuracy (without attack): {np.mean(avg_acc_test) * 100:.4}% | Under attack: {np.mean(avg_acc_ad_test) * 100:.4}%.\")\n",
        "    if attack == mimicry:\n",
        "      del ben_x\n"
      ],
      "metadata": {
        "id": "yRuxHXXjqeOa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your model\n",
        "model_AT_rFGSM = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_rFGSM.load_state_dict(torch.load('best_model%20_RFGSM.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNbO4UcTbDau",
        "outputId": "b97f426a-e8eb-4ad0-ee7a-d9fdde3e35c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv_predict(test_loader, model_DNN, attack=pgd, is_report_loss_diff=False, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hZyGeB3umw4",
        "outputId": "49022b95-dcee-44d7-a3e0-bb1febca0582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 0.0%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv_predict(test_loader, model_AT_rFGSM, attack=pgd, is_report_loss_diff=False, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I9WmuKLbSc4",
        "outputId": "808b07ea-223a-4d31-befd-131aee0c2a0c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.36% | Under attack: 90.83%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Groose\n",
        "attack_params = {\"k\":100, 'is_report_loss_diff':False}\n",
        "adv_predict(test_loader, model_DNN, grosse_k, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, grosse_k, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wONb0rUJRvzW",
        "outputId": "5f74495b-d92f-4537-92d8-7c513d6469ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 0.0%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 83.93%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BCA\n",
        "attack_params = {\"k\":100, 'is_report_loss_diff':False, 'use_sample':False}\n",
        "adv_predict(test_loader, model_DNN, bca_k, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, bca_k, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJGl4w0BSacY",
        "outputId": "ab944b0f-64bc-442f-cb45-b0ff2e344134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 0.0%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 83.93%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BGA\n",
        "attack_params = {\"k\":100, 'is_report_loss_diff':False, 'use_sample':False}\n",
        "adv_predict(test_loader, model_DNN, bga_k, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, bga_k, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smmqoxFBTSBn",
        "outputId": "ee0dc6bf-d1d5-49c9-a90e-90a52a9083b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 0.0%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 90.87%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rFGSM\n",
        "attack_params = {\"k\":100, \"epsilon\":0.02, 'random':True, 'is_report_loss_diff':False, 'is_sample':False}\n",
        "adv_predict(test_loader, model_DNN, dfgsm_k, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, dfgsm_k, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4PPi7cYTn6P",
        "outputId": "c8c0f90f-fb99-46f6-f4c5-45ee46993c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 0.0%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 89.91%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PGD-l1\n",
        "attack_params = {\"k\":500, \"step_length\":1., 'norm':'l1', 'random':False, 'is_report_loss_diff':False, 'is_sample':False}\n",
        "adv_predict(test_loader, model_DNN, pgd, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkyHWVekVDVm",
        "outputId": "2a3d4e03-43d5-4b05-fbb5-9f605a562ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 0.0%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 82.9%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PGD-l2\n",
        "attack_params = {\"k\":200, \"step_length\":0.05, 'norm':'l2', 'random':False, 'is_report_loss_diff':False, 'is_sample':False}\n",
        "adv_predict(test_loader, model_DNN, pgd, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB4Edv2wVnH9",
        "outputId": "b1a62e4c-4ff9-4f10-fea7-36b08bd20c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 40.03%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 91.56%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PGD-linf\n",
        "attack_params = {\"k\":500, \"step_length\":0.002, 'norm':'linf', 'random':False, 'is_report_loss_diff':False, 'is_sample':False}\n",
        "adv_predict(test_loader, model_DNN, pgd, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5noPLfF6V5Wm",
        "outputId": "6f0fcdd7-5592-4de6-e167-bdd6517c410c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 0.0%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 86.74%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mimicry×1\n",
        "attack_params = {\"trials\":1, 'is_report_loss_diff':False}\n",
        "adv_predict(test_loader, model_DNN, mimicry, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, mimicry, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffeGFo4kqm2I",
        "outputId": "d809984c-c855-494a-c64c-217639252a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 84.44%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 92.63%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mimicry×10\n",
        "attack_params = {\"trials\":10, 'is_report_loss_diff':False}\n",
        "adv_predict(test_loader, model_DNN, mimicry, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, mimicry, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcWutrM5qpV9",
        "outputId": "b79c61ee-8f74-4b8a-d8e3-2a07ab83e7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 4.929%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 83.69%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mimicry×30\n",
        "attack_params = {\"trials\":30, 'is_report_loss_diff':False}\n",
        "adv_predict(test_loader, model_DNN, mimicry, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, mimicry, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2uu0nOrWRnM",
        "outputId": "217144f7-0519-47db-fe1d-4b144ae4c91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 4.929%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 83.69%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mimicry×100\n",
        "attack_params = {\"trials\":100, 'is_report_loss_diff':False}\n",
        "adv_predict(test_loader, model_DNN, mimicry, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, mimicry, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfW_K--ErFY-",
        "outputId": "f48a6cd8-10dc-424d-8821-6e8a29d29194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 4.929%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 83.59%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mimicry×1000\n",
        "attack_params = {\"trials\":1000, 'is_report_loss_diff':False}\n",
        "adv_predict(test_loader, model_DNN, mimicry, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, mimicry, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdwkoNeZ_hjk",
        "outputId": "23cfe68e-a7c3-4c07-edcf-d81cea29aa04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 1.517%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 82.68%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mimicry×1000\n",
        "attack_params = {\"trials\":5000, 'is_report_loss_diff':False}\n",
        "adv_predict(test_loader, model_DNN, mimicry, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, mimicry, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTP15xFADZkm",
        "outputId": "158c7f5a-1539-4e25-f01c-83a4713060d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy (without attack): 99.34% | Under attack: 0.0%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 74.55%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params = {'step_lengths':{\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.001}, \"steps\":500}\n",
        "adv_predict(test_loader, model_AT_rFGSM, StepwiseMax_onestep2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "xylm8-33y2uc",
        "outputId": "3d6737b2-ef8f-4c76-d1be-345b057e242b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step-wise max: attack effectiveness 0.000%.\n",
            "step-wise max: attack effectiveness 10.000%.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e8a25759aed8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'step_lengths'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"l1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"linf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madv_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStepwiseMax_onestep2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-b64b9f5a7c87>\u001b[0m in \u001b[0;36madv_predict\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Generate adversarial examples for test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpertb_mal_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/adverserial_attacks_functions.py\u001b[0m in \u001b[0;36mStepwiseMax_onestep2\u001b[0;34m(x, label, model, attack_list, step_lengths, steps, random_start, round_threshold)\u001b[0m\n\u001b[1;32m   1140\u001b[0m       \u001b[0mnum_sample_red\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m       \u001b[0mpertbx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpgd_one_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/adverserial_attacks_functions.py\u001b[0m in \u001b[0;36mpgd_one_step2\u001b[0;34m(x, y, model, step_lengths, x_initial)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0mx_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m     \u001b[0my_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-0d2881a5e67e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define different attacks with their parameters\n",
        "attacks = [\n",
        "    (dfgsm_k, {\"k\":100, \"epsilon\":0.02, 'is_report_loss_diff' : False}),\n",
        "    (bga_k, {\"k\":100, 'is_report_loss_diff' : False}),\n",
        "    (bca_k, {\"k\":100, 'is_report_loss_diff' : False}),\n",
        "    (grosse_k, {\"k\":100, 'is_report_loss_diff' : False}),\n",
        "\n",
        "    (pgd, {'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}),\n",
        "    (pgd, {'k': 200, 'step_length': 0.05, 'norm': 'l2', 'is_report_loss_diff' : False}),\n",
        "    (pgd, {'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}),\n",
        "    (StepwiseMax_onestep2, {'step_lengths':{\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.0008}, \"steps\":650}),\n",
        "\n",
        "    # Add more attacks as needed\n",
        "]\n",
        "\n",
        "# Iterate over each attack and its parameters\n",
        "for attack_func, attack_params in attacks:\n",
        "    print(f\"Running attack: {attack_func.__name__} with parameters: {attack_params}\")\n",
        "    adv_predict(test_loader, model_AT_rFGSM, attack_func, device, **attack_params)\n",
        "    print()  # Print an empty line for separation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JDuv4WBvq54",
        "outputId": "24045f6f-205a-483c-fb74-49842ac7bc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running attack: dfgsm_k with parameters: {'k': 50, 'epsilon': 0.02, 'is_report_loss_diff': False}\n",
            "Adversarial accuracy (without attack): 99.25% | Under attack: 83.43%.\n",
            "\n",
            "Running attack: bga_k with parameters: {'k': 25, 'is_report_loss_diff': False}\n",
            "Adversarial accuracy (without attack): 99.25% | Under attack: 91.57%.\n",
            "\n",
            "Running attack: bca_k with parameters: {'k': 25, 'is_report_loss_diff': False}\n",
            "Adversarial accuracy (without attack): 99.25% | Under attack: 87.93%.\n",
            "\n",
            "Running attack: grosse_k with parameters: {'k': 25, 'is_report_loss_diff': False}\n",
            "Adversarial accuracy (without attack): 99.25% | Under attack: 87.93%.\n",
            "\n",
            "Running attack: pgd with parameters: {'k': 100, 'step_length': 1.0, 'norm': 'l1', 'is_report_loss_diff': False}\n",
            "Adversarial accuracy (without attack): 99.25% | Under attack: 87.24%.\n",
            "\n",
            "Running attack: pgd with parameters: {'k': 200, 'step_length': 0.05, 'norm': 'l2', 'is_report_loss_diff': False}\n",
            "Adversarial accuracy (without attack): 99.25% | Under attack: 91.67%.\n",
            "\n",
            "Running attack: pgd with parameters: {'k': 500, 'step_length': 0.002, 'norm': 'linf', 'is_report_loss_diff': False}\n",
            "Adversarial accuracy (without attack): 99.25% | Under attack: 87.98%.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vpuDgt1W-0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "def mimic_attack_effectiveness_optimized(test_loader, model, seed, trials=1000, device=\"cuda:0\"):\n",
        "  \"\"\"\n",
        "  Calculates the effectiveness of the mimic attack on the given model.\n",
        "\n",
        "  Args:\n",
        "      test_loader: A PyTorch dataloader containing the test data.\n",
        "      model: The PyTorch model to be attacked.\n",
        "      seed: The random seed for reproducibility.\n",
        "      trials: The number of random samples to use from the benign class (default: 1000).\n",
        "      device: The device to use for computations (default: \"cuda:0\" if available, otherwise \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "      The effectiveness of the mimic attack as a percentage (float).\n",
        "  \"\"\"\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize counters\n",
        "  successful_attacks = 0\n",
        "  total_malicious_samples = 0\n",
        "\n",
        "  # Pre-select benign samples for efficiency\n",
        "  benign_samples = []\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "  ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "  # Clear unnecessary variables\n",
        "  del benign_samples\n",
        "\n",
        "  trials = min(trials, len(ben_x))\n",
        "\n",
        "\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    malicious_samples = x_batch[y_batch.squeeze() == 1]\n",
        "\n",
        "    if len(malicious_samples) > 0:\n",
        "      # Expand dimensions for efficient broadcasting\n",
        "      malicious_samples_expanded = malicious_samples.unsqueeze(1).expand(-1, trials, -1)\n",
        "\n",
        "      # Generate random indices outside the loop\n",
        "      seed += 1\n",
        "      torch.manual_seed(seed)\n",
        "      indices = torch.randperm(len(ben_x), device=device)[:trials]\n",
        "      trial_vectors_expanded = ben_x[indices].unsqueeze(0)\n",
        "\n",
        "      # Perform the mimic attack and update counters\n",
        "      modified_x = torch.clamp(malicious_samples_expanded + trial_vectors_expanded, min=0., max=1.)\n",
        "      _, done = get_loss(modified_x.view(-1, modified_x.shape[-1]), torch.ones(trials * malicious_samples.shape[0], 1, device=device), model)\n",
        "      successful_attacks += (done.view(malicious_samples.shape[0], trials).sum(dim=1) > 0).sum().item()\n",
        "      total_malicious_samples += malicious_samples.shape[0]\n",
        "\n",
        "  # Calculate and print attack effectiveness\n",
        "  attack_effectiveness = (successful_attacks / total_malicious_samples) * 100 if total_malicious_samples > 0 else 0\n",
        "  print(f\"Mimic attack effectiveness: {attack_effectiveness:.3f}%.\")\n",
        "\n",
        "  return attack_effectiveness  # Added return statement for clarity\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kVA4nOM0YUVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mimicry(ben_x, malicious_samples, model_DNN, trials=30, seed=230, is_report_loss_diff=False):\n",
        "    \"\"\"\n",
        "    Perform a mimicry attack.\n",
        "\n",
        "    Args:\n",
        "    - ben_x (torch.Tensor): Benign samples tensor.\n",
        "    - malicious_samples (torch.Tensor): Malicious samples tensor.\n",
        "    - model_DNN (torch.nn.Module): PyTorch model used for the attack.\n",
        "    - trials (int): Number of trials for the attack.\n",
        "    - seed (int): Random seed for reproducibility.\n",
        "    - is_report_loss_diff (bool): Flag to indicate whether to report attack effectiveness.\n",
        "\n",
        "    Returns:\n",
        "    - adv_x (torch.Tensor): Adversarial examples tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure trials do not exceed the length of ben_x\n",
        "    trials = min(trials, len(ben_x))\n",
        "\n",
        "    # Get the number of malicious samples\n",
        "    n_samples = len(malicious_samples)\n",
        "\n",
        "    if n_samples > 0:\n",
        "        # Expand dimensions for efficient broadcasting\n",
        "        malicious_samples_expanded = malicious_samples.unsqueeze(1).expand(-1, trials, -1)\n",
        "\n",
        "        # Generate random indices for sampling from ben_x\n",
        "        torch.manual_seed(seed)\n",
        "        indices = torch.randperm(len(ben_x), device=ben_x.device)[:trials]\n",
        "        trial_vectors_expanded = ben_x[indices].unsqueeze(0)\n",
        "\n",
        "        # Perform the mimic attack\n",
        "        pertbx = torch.clamp(malicious_samples_expanded + trial_vectors_expanded, min=0., max=1.)\n",
        "\n",
        "        # Compute the loss and check if adversarial examples are successful\n",
        "        loss, done = get_loss(pertbx.view(-1, pertbx.shape[-1]), torch.ones(n_samples * trials, 1, device=ben_x.device), model_DNN)\n",
        "\n",
        "        # Add maximum loss to successful attacks to differentiate\n",
        "        max_v = loss.max()\n",
        "        loss[done] += max_v\n",
        "\n",
        "        # Reshape the loss and done tensors\n",
        "        loss = loss.view(n_samples, trials)\n",
        "        done = done.view(n_samples, trials)\n",
        "\n",
        "        # Report attack effectiveness if required\n",
        "        if is_report_loss_diff:\n",
        "            n_done = torch.any(done, dim=-1).sum()\n",
        "            print(f\"Mimicry*{trials}: Attack effectiveness {n_done / n_samples * 100:.3f}%.\")\n",
        "\n",
        "        # Get the index of the maximum loss for each sample\n",
        "        _, indices = loss.max(dim=-1)\n",
        "        adv_x = pertbx[torch.arange(n_samples), indices]\n",
        "\n",
        "        del pertbx, loss, done, malicious_samples_expanded, trial_vectors_expanded\n",
        "\n",
        "        return adv_x\n",
        "    else:\n",
        "        print(\"No malicious samples found.\")\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iL-Dn7PrhtZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mimicry×30\n",
        "attack_params = {\"trials\":10, 'is_report_loss_diff':True}\n",
        "#adv_predict(test_loader, model_DNN, mimicry, device, **attack_params)\n",
        "adv_predict(test_loader, model_AT_rFGSM, mimicry, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d98baa4-a59b-4fe2-bdc9-f3df5653dfef",
        "id": "cGzkqvHCqD-P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 33.333%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 10.526%.\n",
            "Mimicry*10: Attack effectiveness 40.000%.\n",
            "Mimicry*10: Attack effectiveness 37.500%.\n",
            "Mimicry*10: Attack effectiveness 18.750%.\n",
            "Mimicry*10: Attack effectiveness 12.500%.\n",
            "Mimicry*10: Attack effectiveness 7.692%.\n",
            "Mimicry*10: Attack effectiveness 23.529%.\n",
            "Mimicry*10: Attack effectiveness 30.769%.\n",
            "Mimicry*10: Attack effectiveness 21.429%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 17.647%.\n",
            "Mimicry*10: Attack effectiveness 36.842%.\n",
            "Mimicry*10: Attack effectiveness 8.333%.\n",
            "Mimicry*10: Attack effectiveness 12.500%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 22.222%.\n",
            "Mimicry*10: Attack effectiveness 23.077%.\n",
            "Mimicry*10: Attack effectiveness 11.111%.\n",
            "Mimicry*10: Attack effectiveness 33.333%.\n",
            "Mimicry*10: Attack effectiveness 11.111%.\n",
            "Mimicry*10: Attack effectiveness 12.500%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 15.385%.\n",
            "Mimicry*10: Attack effectiveness 9.091%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 7.143%.\n",
            "Mimicry*10: Attack effectiveness 15.385%.\n",
            "Mimicry*10: Attack effectiveness 7.143%.\n",
            "Mimicry*10: Attack effectiveness 35.714%.\n",
            "Mimicry*10: Attack effectiveness 15.385%.\n",
            "Mimicry*10: Attack effectiveness 15.385%.\n",
            "Mimicry*10: Attack effectiveness 8.333%.\n",
            "Mimicry*10: Attack effectiveness 16.667%.\n",
            "Mimicry*10: Attack effectiveness 11.111%.\n",
            "Mimicry*10: Attack effectiveness 11.111%.\n",
            "Mimicry*10: Attack effectiveness 27.273%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 23.529%.\n",
            "Mimicry*10: Attack effectiveness 26.667%.\n",
            "Mimicry*10: Attack effectiveness 26.667%.\n",
            "Mimicry*10: Attack effectiveness 25.000%.\n",
            "Mimicry*10: Attack effectiveness 9.091%.\n",
            "Mimicry*10: Attack effectiveness 16.667%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 9.091%.\n",
            "Mimicry*10: Attack effectiveness 27.273%.\n",
            "Mimicry*10: Attack effectiveness 33.333%.\n",
            "Mimicry*10: Attack effectiveness 27.273%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 8.333%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 11.111%.\n",
            "Mimicry*10: Attack effectiveness 16.667%.\n",
            "Mimicry*10: Attack effectiveness 12.500%.\n",
            "Mimicry*10: Attack effectiveness 18.182%.\n",
            "Mimicry*10: Attack effectiveness 42.857%.\n",
            "Mimicry*10: Attack effectiveness 42.857%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 33.333%.\n",
            "Mimicry*10: Attack effectiveness 14.286%.\n",
            "Mimicry*10: Attack effectiveness 18.182%.\n",
            "Mimicry*10: Attack effectiveness 7.692%.\n",
            "Mimicry*10: Attack effectiveness 27.273%.\n",
            "Mimicry*10: Attack effectiveness 23.077%.\n",
            "Mimicry*10: Attack effectiveness 14.286%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 16.667%.\n",
            "Mimicry*10: Attack effectiveness 12.500%.\n",
            "Mimicry*10: Attack effectiveness 16.667%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 28.571%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 7.692%.\n",
            "Mimicry*10: Attack effectiveness 9.091%.\n",
            "Mimicry*10: Attack effectiveness 22.222%.\n",
            "Mimicry*10: Attack effectiveness 13.333%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 16.667%.\n",
            "Mimicry*10: Attack effectiveness 33.333%.\n",
            "Mimicry*10: Attack effectiveness 25.000%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 14.286%.\n",
            "Mimicry*10: Attack effectiveness 21.429%.\n",
            "Mimicry*10: Attack effectiveness 22.222%.\n",
            "Mimicry*10: Attack effectiveness 6.250%.\n",
            "Mimicry*10: Attack effectiveness 9.091%.\n",
            "Mimicry*10: Attack effectiveness 6.667%.\n",
            "Mimicry*10: Attack effectiveness 30.000%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 20.000%.\n",
            "Mimicry*10: Attack effectiveness 0.000%.\n",
            "Mimicry*10: Attack effectiveness 42.857%.\n",
            "Mimicry*10: Attack effectiveness 12.500%.\n",
            "Adversarial accuracy (without attack): 99.36% | Under attack: 83.35%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x_test, y_test in test_loader:\n",
        "\n",
        "        # Generate adversarial examples for test set\n",
        "        mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "\n",
        "        pertb_mal_x = mimicry(ben_x, mal_x_batch, model_AT_rFGSM, trials=30, seed=230, is_report_loss_diff=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpl1HyDQnzjf",
        "outputId": "8d3922c9-e1e4-42db-f382-582af9f79e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 33.333%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 10.526%.\n",
            "Mimicry*30: Attack effectiveness 40.000%.\n",
            "Mimicry*30: Attack effectiveness 37.500%.\n",
            "Mimicry*30: Attack effectiveness 18.750%.\n",
            "Mimicry*30: Attack effectiveness 12.500%.\n",
            "Mimicry*30: Attack effectiveness 7.692%.\n",
            "Mimicry*30: Attack effectiveness 23.529%.\n",
            "Mimicry*30: Attack effectiveness 30.769%.\n",
            "Mimicry*30: Attack effectiveness 21.429%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 17.647%.\n",
            "Mimicry*30: Attack effectiveness 36.842%.\n",
            "Mimicry*30: Attack effectiveness 8.333%.\n",
            "Mimicry*30: Attack effectiveness 12.500%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 22.222%.\n",
            "Mimicry*30: Attack effectiveness 23.077%.\n",
            "Mimicry*30: Attack effectiveness 11.111%.\n",
            "Mimicry*30: Attack effectiveness 33.333%.\n",
            "Mimicry*30: Attack effectiveness 11.111%.\n",
            "Mimicry*30: Attack effectiveness 12.500%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 15.385%.\n",
            "Mimicry*30: Attack effectiveness 9.091%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 7.143%.\n",
            "Mimicry*30: Attack effectiveness 15.385%.\n",
            "Mimicry*30: Attack effectiveness 7.143%.\n",
            "Mimicry*30: Attack effectiveness 35.714%.\n",
            "Mimicry*30: Attack effectiveness 15.385%.\n",
            "Mimicry*30: Attack effectiveness 15.385%.\n",
            "Mimicry*30: Attack effectiveness 8.333%.\n",
            "Mimicry*30: Attack effectiveness 16.667%.\n",
            "Mimicry*30: Attack effectiveness 11.111%.\n",
            "Mimicry*30: Attack effectiveness 11.111%.\n",
            "Mimicry*30: Attack effectiveness 27.273%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 23.529%.\n",
            "Mimicry*30: Attack effectiveness 26.667%.\n",
            "Mimicry*30: Attack effectiveness 26.667%.\n",
            "Mimicry*30: Attack effectiveness 25.000%.\n",
            "Mimicry*30: Attack effectiveness 9.091%.\n",
            "Mimicry*30: Attack effectiveness 16.667%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 9.091%.\n",
            "Mimicry*30: Attack effectiveness 27.273%.\n",
            "Mimicry*30: Attack effectiveness 33.333%.\n",
            "Mimicry*30: Attack effectiveness 27.273%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 8.333%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 11.111%.\n",
            "Mimicry*30: Attack effectiveness 16.667%.\n",
            "Mimicry*30: Attack effectiveness 12.500%.\n",
            "Mimicry*30: Attack effectiveness 18.182%.\n",
            "Mimicry*30: Attack effectiveness 42.857%.\n",
            "Mimicry*30: Attack effectiveness 42.857%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 33.333%.\n",
            "Mimicry*30: Attack effectiveness 14.286%.\n",
            "Mimicry*30: Attack effectiveness 18.182%.\n",
            "Mimicry*30: Attack effectiveness 7.692%.\n",
            "Mimicry*30: Attack effectiveness 27.273%.\n",
            "Mimicry*30: Attack effectiveness 23.077%.\n",
            "Mimicry*30: Attack effectiveness 14.286%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 16.667%.\n",
            "Mimicry*30: Attack effectiveness 12.500%.\n",
            "Mimicry*30: Attack effectiveness 16.667%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 28.571%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 7.692%.\n",
            "Mimicry*30: Attack effectiveness 9.091%.\n",
            "Mimicry*30: Attack effectiveness 22.222%.\n",
            "Mimicry*30: Attack effectiveness 13.333%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 16.667%.\n",
            "Mimicry*30: Attack effectiveness 33.333%.\n",
            "Mimicry*30: Attack effectiveness 25.000%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 14.286%.\n",
            "Mimicry*30: Attack effectiveness 21.429%.\n",
            "Mimicry*30: Attack effectiveness 22.222%.\n",
            "Mimicry*30: Attack effectiveness 6.250%.\n",
            "Mimicry*30: Attack effectiveness 9.091%.\n",
            "Mimicry*30: Attack effectiveness 6.667%.\n",
            "Mimicry*30: Attack effectiveness 30.000%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 20.000%.\n",
            "Mimicry*30: Attack effectiveness 0.000%.\n",
            "Mimicry*30: Attack effectiveness 42.857%.\n",
            "Mimicry*30: Attack effectiveness 12.500%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trials = 30\n",
        "seed = 230\n",
        "is_report_loss_diff = True\n",
        "\n",
        "model_DNN.eval()\n",
        "\n",
        "# Ensure trials do not exceed the length of ben_x\n",
        "trials = min(trials, len(ben_x))\n",
        "\n",
        "# Get the number of malicious samples\n",
        "n_samples = len(malicious_samples)\n",
        "\n",
        "if n_samples > 0:\n",
        "    # Expand dimensions for efficient broadcasting\n",
        "    malicious_samples_expanded = malicious_samples.unsqueeze(1).expand(-1, trials, -1)\n",
        "\n",
        "    # Generate random indices for sampling from ben_x\n",
        "    seed += 1\n",
        "    torch.manual_seed(seed)\n",
        "    indices = torch.randperm(len(ben_x), device=device)[:trials]\n",
        "    trial_vectors_expanded = ben_x[indices].unsqueeze(0)\n",
        "\n",
        "    # Perform the mimic attack\n",
        "    pertbx = torch.clamp(malicious_samples_expanded + trial_vectors_expanded, min=0., max=1.)\n",
        "\n",
        "    # Compute the loss and check if adversarial examples are successful\n",
        "    loss, done = get_loss(pertbx.view(-1, pertbx.shape[-1]), torch.ones(n_samples * trials, 1, device=device), model_DNN)\n",
        "    print(done)\n",
        "    # Add maximum loss to successful attacks to differentiate\n",
        "    max_v = loss.max()\n",
        "    loss[done] += max_v\n",
        "\n",
        "    # Reshape the loss and done tensors\n",
        "    loss = loss.view(n_samples, trials)\n",
        "    done = done.view(n_samples, trials)\n",
        "\n",
        "    # Report attack effectiveness if required\n",
        "    if is_report_loss_diff:\n",
        "        n_done = torch.any(done, dim=-1).sum()\n",
        "        print(n_done)\n",
        "        print(f\"Mimicry*{trials}: Attack effectiveness {n_done / n_samples * 100:.3f}%.\")\n",
        "\n",
        "    # Get the index of the maximum loss for each sample\n",
        "    _, indices = loss.max(dim=-1)\n",
        "    adv_x = pertbx[torch.arange(n_samples), indices]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIilgMw1cWuE",
        "outputId": "9a454514-da5b-4e0f-c409-c0e430a79410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "         True,  True, False, False, False,  True, False, False, False, False,\n",
            "         True, False, False, False, False, False, False,  True, False, False,\n",
            "         True, False, False,  True,  True, False,  True, False, False, False,\n",
            "         True,  True, False,  True, False,  True, False, False,  True, False,\n",
            "         True, False,  True,  True, False, False,  True,  True, False,  True,\n",
            "        False, False, False, False,  True, False, False, False, False, False,\n",
            "         True,  True, False, False, False,  True, False, False, False, False,\n",
            "         True, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False, False,  True,  True, False,  True, False, False,  True,\n",
            "         True,  True, False,  True, False,  True, False, False,  True, False,\n",
            "         True, False,  True,  True, False, False, False,  True, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False, False, False,  True, False, False, False, False, False,\n",
            "         True,  True, False, False, False,  True, False, False,  True, False,\n",
            "         True, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "         True,  True, False, False, False,  True, False, False, False, False,\n",
            "         True, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False, False, False,  True, False, False, False, False, False,\n",
            "         True,  True, False, False, False,  True, False, False,  True, False,\n",
            "         True, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False,  True, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False, False, False,  True, False, False, False, False, False,\n",
            "         True,  True, False, False, False,  True, False, False,  True, False,\n",
            "         True, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False, False, False,  True, False, False, False, False, False,\n",
            "         True,  True, False, False, False,  True, False, False,  True, False,\n",
            "         True, False, False, False, False, False, False,  True, False, False])\n",
            "tensor(11)\n",
            "Mimicry*30: Attack effectiveness 91.667%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adv_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7QK3DYDlkag",
        "outputId": "6aa17073-9d33-4c1b-c745-50894efc612e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "trials = 30\n",
        "seed = 230\n",
        "is_report_loss_diff = True\n",
        "\n",
        "adv_x = mimicry(ben_x, malicious_samples, model_DNN, trials, seed, is_report_loss_diff, device=\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbuPZffKiWkP",
        "outputId": "1967238c-7a55-4394-9020-6de24138b670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mimicry*30: Attack effectiveness 91.667%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model_AT_rFGSM(adv_x)\n",
        "_, y_pred = torch.topk(outputs, k=1)\n",
        "\n",
        "acc_ad_test = (y_pred == 1.).sum().item() / len(y_pred)\n",
        "acc_ad_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgLF4XbClzmr",
        "outputId": "fa53553c-d55e-41f6-b303-1e9170648e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mimicry(ben_x, malicious_samples, model, seed, trials=1000, device=\"cpu\"):\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize counters\n",
        "  successful_attacks = 0\n",
        "  total_malicious_samples = 0\n",
        "\n",
        "  trials = min(trials, len(ben_x))\n",
        "\n",
        "\n",
        "  if len(malicious_samples) > 0:\n",
        "\n",
        "    # Generate random indices outside the loop\n",
        "    seed += 1\n",
        "    torch.manual_seed(seed)\n",
        "    indices = torch.randperm(len(ben_x), device=device)[:trials]\n",
        "    trial_vectors = ben_x[indices]\n",
        "\n",
        "    # Perform the mimic attack and update counters\n",
        "    modified_x = torch.clamp(malicious_samples + trial_vectors, min=0., max=1.)\n",
        "    _, done = get_loss(modified_x.view(-1, modified_x.shape[-1]), torch.ones(trials * malicious_samples.shape[0], 1, device=device), model)\n",
        "    successful_attacks += (done.view(malicious_samples.shape[0], trials).sum(dim=1) > 0).sum().item()\n",
        "    total_malicious_samples += malicious_samples.shape[0]\n",
        "\n",
        "  # Calculate and print attack effectiveness\n",
        "  attack_effectiveness = (successful_attacks / total_malicious_samples) * 100 if total_malicious_samples > 0 else 0\n",
        "  print(f\"Mimic attack effectiveness: {attack_effectiveness:.3f}%.\")\n",
        "\n",
        "  return attack_effectiveness  # Added return statement for clarity\n"
      ],
      "metadata": {
        "id": "ycRVXx-1YwjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mimic_attack_effectiveness_optimized(test_loader, model, seed, trials=1000, device=\"cuda:0\"):\n",
        "  \"\"\"\n",
        "  Calculates the effectiveness of the mimic attack on the given model.\n",
        "\n",
        "  Args:\n",
        "      test_loader: A PyTorch dataloader containing the test data.\n",
        "      model: The PyTorch model to be attacked.\n",
        "      seed: The random seed for reproducibility.\n",
        "      trials: The number of random samples to use from the benign class (default: 1000).\n",
        "      device: The device to use for computations (default: \"cuda:0\" if available, otherwise \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "      The effectiveness of the mimic attack as a percentage (float).\n",
        "  \"\"\"\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize counters\n",
        "  successful_attacks = 0\n",
        "  total_malicious_samples = 0\n",
        "\n",
        "  # Pre-select benign samples for efficiency\n",
        "  benign_samples = []\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "  ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "  # Clear unnecessary variables\n",
        "  del benign_samples\n",
        "\n",
        "  trials = min(trials, len(ben_x))\n",
        "\n",
        "\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    malicious_samples = x_batch[y_batch.squeeze() == 1]\n",
        "\n",
        "    if len(malicious_samples) > 0:\n",
        "      # Expand dimensions for efficient broadcasting\n",
        "      malicious_samples_expanded = malicious_samples.unsqueeze(1).expand(-1, trials, -1)\n",
        "\n",
        "      # Generate random indices outside the loop\n",
        "      seed += 1\n",
        "      torch.manual_seed(seed)\n",
        "      indices = torch.randperm(len(ben_x), device=device)[:trials]\n",
        "      trial_vectors_expanded = ben_x[indices].unsqueeze(0)\n",
        "\n",
        "      # Perform the mimic attack and update counters\n",
        "      modified_x = torch.clamp(malicious_samples_expanded + trial_vectors_expanded, min=0., max=1.)\n",
        "      _, done = get_loss(modified_x.view(-1, modified_x.shape[-1]), torch.ones(trials * malicious_samples.shape[0], 1, device=device), model)\n",
        "      successful_attacks += (done.view(malicious_samples.shape[0], trials).sum(dim=1) > 0).sum().item()\n",
        "      total_malicious_samples += malicious_samples.shape[0]\n",
        "      break\n",
        "  # Calculate and print attack effectiveness\n",
        "  attack_effectiveness = (successful_attacks / total_malicious_samples) * 100 if total_malicious_samples > 0 else 0\n",
        "  print(f\"Mimic attack effectiveness: {attack_effectiveness:.3f}%.\")\n",
        "\n",
        "  return attack_effectiveness  # Added return statement for clarity\n"
      ],
      "metadata": {
        "id": "NpnhzTu07Aep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mimic_attack_effectiveness_optimized(test_loader, model_DNN , seed=230, trials=100, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iemV-2EK6wng",
        "outputId": "70f11528-5e96-447d-93ec-979c22740b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mimic attack effectiveness: 91.667%.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.66666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ]
}