{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMbaY9mIYvYLnxnoVcdao5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/mal_adv3/blob/main/adverserial_attacks_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import sparse\n",
        "import gdown\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "import random"
      ],
      "metadata": {
        "id": "G-8aXXZ9Be6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def round_x(x, round_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Rounds x by thresholding it according to round_threshold.\n",
        "    :param x: input tensor\n",
        "    :param round_threshold: threshold parameter\n",
        "    :return: a tensor of 0s and 1s\n",
        "    \"\"\"\n",
        "    return (x >= round_threshold).float()\n",
        "\n",
        "def get_x0(x, initial_rounding_threshold=0.5, is_sample=False):\n",
        "    \"\"\"\n",
        "    Helper function to randomly initialize the inner maximizer algorithm.\n",
        "    Randomizes the input tensor while preserving its functionality.\n",
        "    :param x: input tensor\n",
        "    :param rounding_threshold: threshold for rounding\n",
        "    :param is_sample: flag to sample randomly from feasible area\n",
        "    :return: randomly sampled feasible version of x\n",
        "    \"\"\"\n",
        "    if is_sample:\n",
        "        rand_x = round_x(torch.rand(x.size()), initial_rounding_threshold=initial_rounding_threshold)\n",
        "        return (rand_x.byte() | x.byte()).float()\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "def or_float_tensors(x_1, x_2):\n",
        "    \"\"\"\n",
        "    ORs two float tensors by converting them to byte and back.\n",
        "    :param x_1: tensor one\n",
        "    :param x_2: tensor two\n",
        "    :return: float tensor of 0s and 1s\n",
        "    \"\"\"\n",
        "    return (x_1.byte() | x_2.byte()).float()\n",
        "\n",
        "\n",
        "def xor_float_tensors(x_1, x_2):\n",
        "    \"\"\"\n",
        "    XORs two float tensors by converting them to byte and back\n",
        "    Note that byte() takes the first 8 bit after the decimal point of the float\n",
        "    e.g., 0.0 ==> 0\n",
        "          0.1 ==> 0\n",
        "          1.1 ==> 1\n",
        "        255.1 ==> 255\n",
        "        256.1 ==> 0\n",
        "    Subsequently the purpose of this function is to map 1s float tensors to 1\n",
        "    and those of 0s to 0. I.e., it is meant to be used on tensors of 0s and 1s.\n",
        "\n",
        "    :param x_1: tensor one\n",
        "    :param x_2: tensor two\n",
        "    :return: float tensor of 0s and 1s.\n",
        "    \"\"\"\n",
        "    return (x_1.byte() ^ x_2.byte()).float()\n",
        "\n",
        "def get_loss(x,y,model):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(x)\n",
        "    loss = criterion(outputs, y.view(-1).long())\n",
        "    _, predicted = torch.topk(outputs, k=1)\n",
        "    done = (predicted != y).squeeze()\n",
        "\n",
        "    return loss, done\n",
        "\n"
      ],
      "metadata": {
        "id": "XCbqeJ84gfDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dfgsm_k(x, y, model, k=25, epsilon=0.02, alpha=1., initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    FGSM^k with deterministic rounding\n",
        "    :param y: ground truth labels\n",
        "    :param x: feature vector\n",
        "    :param model: neural network model\n",
        "    :param k: number of steps\n",
        "    :param epsilon: update value in each direction\n",
        "    :param alpha: hyperparameter for controlling the portionate of rounding\n",
        "    :param initial_rounding_threshold: threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: threshold parameter for rounding\n",
        "    :param is_report_loss_diff: flag to report loss difference\n",
        "    :param is_sample: flag to sample randomly from the feasible area\n",
        "    :return: the adversarial version of x according to dfgsm_k (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        # Find the next sample\n",
        "        x_next = x_next + epsilon * torch.sign(grad_vars[0].data)\n",
        "\n",
        "        # Projection\n",
        "        x_next = torch.clamp(x_next, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = (torch.rand(x_next.size()) * alpha).to(device=x.device)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        #print(f\"Natural loss: {loss_natural.mean():.4f}, Adversarial loss: {loss_adv.mean():.4f}, Difference: {(loss_adv.mean() - loss_natural.mean()):.4f}\")\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"dFGSM: attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next\n"
      ],
      "metadata": {
        "id": "H6BiF-vCHLlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bga_k(x, y, model, k=25, alpha=1., is_report_loss_diff=True, use_sample=False):\n",
        "    \"\"\"\n",
        "    Multi-step bit gradient ascent\n",
        "    :param x: feature vector\n",
        "    :param y: ground truth labels\n",
        "    :param model: neural network model\n",
        "    :param k: number of steps\n",
        "    :param alpha: hyperparameter for controlling updates\n",
        "    :param is_report_loss_diff: flag to report loss difference\n",
        "    :param use_sample: flag to sample randomly from the feasible area\n",
        "    :return: the adversarial version of x according to bga_k (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize worst loss and corresponding adversarial samples\n",
        "    loss_worst = loss_natural.clone()\n",
        "    x_worst = x.clone()\n",
        "\n",
        "    # Book-keeping\n",
        "    sqrt_m = (torch.sqrt(torch.tensor([x.size()[1]], dtype=torch.float))).to(x.device)\n",
        "\n",
        "    # Multi-step with gradients\n",
        "    for t in range(k):\n",
        "        if t == 0:\n",
        "            # Initialize starting point\n",
        "            x_next = get_x0(x, use_sample)\n",
        "        else:\n",
        "            # Compute gradient\n",
        "            grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "            grad_data = grad_vars[0].data\n",
        "\n",
        "            # Compute the updates\n",
        "            # torch.norm(grad_data, 2, 1), 2:the L2-norm , 1:the norm along dimension 1\n",
        "            x_update = (sqrt_m * (1. - 2. * x_next) * grad_data >= (alpha * torch.norm(grad_data, 2, 1).unsqueeze(1))).float()\n",
        "\n",
        "            # Find the next sample with projection to the feasible set\n",
        "            x_next = xor_float_tensors(x_update, x_next)\n",
        "            x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # Update worst loss and adversarial samples\n",
        "        replace_flag = (loss.data > loss_worst)\n",
        "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
        "        x_worst[replace_flag] = x_next[replace_flag]\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        #print(f\"Natural loss: {loss_natural.mean():.4f}, Adversarial loss: {loss_worst.mean():.4f}, Difference: {(loss_worst.mean() - loss_natural.mean()):.4f}\")\n",
        "        outputs = model(x_worst)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"bga_k: attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    return x_worst\n"
      ],
      "metadata": {
        "id": "V_1x56e5UYFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bca_k(x, y, model, k=25, is_report_loss_diff=True, use_sample=False):\n",
        "    \"\"\"\n",
        "    Multi-step bit coordinate ascent\n",
        "    :param use_sample:\n",
        "    :param is_report_loss_diff:\n",
        "    :param y:\n",
        "    :param x: (tensor) feature vector\n",
        "    :param model: nn model\n",
        "    :param k: num of steps\n",
        "    :return: the adversarial version of x according to bca_k (tensor)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # keeping worst loss\n",
        "    loss_worst = loss_natural.clone()\n",
        "    x_worst = x.clone()\n",
        "\n",
        "    # multi-step with gradients\n",
        "    loss = None\n",
        "    x_var = None\n",
        "    x_next = None\n",
        "    for t in range(k):\n",
        "        if t == 0:\n",
        "            # initialize starting point\n",
        "            x_next = get_x0(x, use_sample)\n",
        "        else:\n",
        "            # compute gradient\n",
        "            grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "            grad_data = grad_vars[0].data\n",
        "\n",
        "            # compute the updates (can be made more efficient than this)\n",
        "            #aug_grad = (1. - 2. * x_next) * grad_data #this line is wrong because the grad_data can be negative\n",
        "            aug_grad = (x_next < 0.5) * grad_data # the correct version\n",
        "            val, _ = torch.topk(aug_grad, 1)\n",
        "            x_update = (aug_grad >= val.expand_as(aug_grad)).float()\n",
        "            # find the next sample with projection to the feasible set\n",
        "            x_next = xor_float_tensors(x_update, x_next)\n",
        "            x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "        # forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # update worst loss and adversarial samples\n",
        "        replace_flag = (loss.data > loss_worst)\n",
        "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
        "        x_worst[replace_flag] = x_next[replace_flag]\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        #print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %(loss_natural.mean(), loss_worst.mean(), loss_worst.mean() - loss_natural.mean()))\n",
        "        outputs = model(x_worst)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"bca_k: attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "\n",
        "\n",
        "    return x_worst"
      ],
      "metadata": {
        "id": "l5w-UPPVeDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grosse_k(x, y, model, k=25, is_report_loss_diff=True, use_sample=False):\n",
        "    \"\"\"\n",
        "    Multi-step bit coordinate ascent\n",
        "    :param use_sample:\n",
        "    :param is_report_loss_diff:\n",
        "    :param y:\n",
        "    :param x: (tensor) feature vector\n",
        "    :param model: nn model\n",
        "    :param k: num of steps\n",
        "    :return: the adversarial version of x according to bca_k (tensor)\n",
        "    \"\"\"\n",
        "    epsilon = 1e-10 # avoid gradient less than epsilon\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # keeping worst loss\n",
        "    loss_worst = loss_natural.clone()\n",
        "    x_worst = x.clone()\n",
        "\n",
        "    # multi-step with gradients\n",
        "    output = None\n",
        "    x_var = None\n",
        "    x_next = None\n",
        "    for t in range(k):\n",
        "        if t == 0:\n",
        "            # initialize starting point\n",
        "            x_next = get_x0(x, use_sample)\n",
        "        else:\n",
        "            # compute gradient\n",
        "            # ouput.shape=([batch_size, 2]) because of 2 neoruns, so we just use the output of the first neorun(benign)\n",
        "            grad_vars = torch.autograd.grad(output[:, 0].mean(), x_var)\n",
        "            grad_data = grad_vars[0].data\n",
        "\n",
        "            # compute the updates (can be made more efficient than this)\n",
        "            #aug_grad = (1. - x_next) * grad_data\n",
        "            aug_grad = (x_next < 0.5) * grad_data\n",
        "            val, _ = torch.topk(aug_grad, 1)\n",
        "            x_update = ((aug_grad >= val.expand_as(aug_grad)).float()) * (aug_grad > epsilon)\n",
        "\n",
        "            # find the next sample with projection to the feasible set\n",
        "            x_next = xor_float_tensors(x_update, x_next)\n",
        "            x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "        # forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        output = model(x_var)\n",
        "        loss = criterion(output, y.view(-1).long())\n",
        "\n",
        "        # update worst loss and adversarial samples\n",
        "        replace_flag = (loss.data > loss_worst)\n",
        "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
        "        x_worst[replace_flag] = x_next[replace_flag]\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        #print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %(loss_natural.mean(), loss_worst.mean(), loss_worst.mean() - loss_natural.mean()))\n",
        "        outputs = model(x_worst)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"grosse_k: attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "\n",
        "\n",
        "    return x_worst"
      ],
      "metadata": {
        "id": "2jjbi-wHwYKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd(x, y, model, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        grad_data = grad_vars[0].data\n",
        "        gradients = grad_data * (x < 0.5)\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients)\n",
        "        elif norm == 'l2_2':\n",
        "            max_grad, _ = gradients.max(dim=1, keepdim=True)\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print(max_grad/l2norm)\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / max_grad)\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm)\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "        elif norm == 'l1':\n",
        "            #ignore the gradient of indice which is updated\n",
        "            gradients = gradients * (x_next < 0.5)\n",
        "            val, _ = torch.topk(gradients, 1)\n",
        "            perturbation = (gradients >= val.expand_as(gradients)).float()\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            outputs = model(x_next)\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            done = (predicted != y).squeeze()\n",
        "\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next\n"
      ],
      "metadata": {
        "id": "j0HTVirP5Gtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "def mimic_attack_effectiveness_optimized(test_loader, model, seed, trials=1000, device=\"cuda:0\"):\n",
        "  \"\"\"\n",
        "  Calculates the effectiveness of the mimic attack on the given model.\n",
        "\n",
        "  Args:\n",
        "      test_loader: A PyTorch dataloader containing the test data.\n",
        "      model: The PyTorch model to be attacked.\n",
        "      seed: The random seed for reproducibility.\n",
        "      trials: The number of random samples to use from the benign class (default: 1000).\n",
        "      device: The device to use for computations (default: \"cuda:0\" if available, otherwise \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "      The effectiveness of the mimic attack as a percentage (float).\n",
        "  \"\"\"\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize counters\n",
        "  successful_attacks = 0\n",
        "  total_malicious_samples = 0\n",
        "\n",
        "  # Pre-select benign samples for efficiency\n",
        "  benign_samples = []\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "  ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "  # Clear unnecessary variables\n",
        "  del benign_samples\n",
        "\n",
        "  trials = min(trials, len(ben_x))\n",
        "\n",
        "\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    malicious_samples = x_batch[y_batch.squeeze() == 1]\n",
        "\n",
        "    if len(malicious_samples) > 0:\n",
        "      # Expand dimensions for efficient broadcasting\n",
        "      malicious_samples_expanded = malicious_samples.unsqueeze(1).expand(-1, trials, -1)\n",
        "\n",
        "      # Generate random indices outside the loop\n",
        "      seed += 1\n",
        "      torch.manual_seed(seed)\n",
        "      indices = torch.randperm(len(ben_x), device=device)[:trials]\n",
        "      trial_vectors_expanded = ben_x[indices].unsqueeze(0)\n",
        "\n",
        "      # Perform the mimic attack and update counters\n",
        "      modified_x = torch.clamp(malicious_samples_expanded + trial_vectors_expanded, min=0., max=1.)\n",
        "      _, done = get_loss(modified_x.view(-1, modified_x.shape[-1]), torch.ones(trials * malicious_samples.shape[0], 1, device=device), model)\n",
        "      successful_attacks += (done.view(malicious_samples.shape[0], trials).sum(dim=1) > 0).sum().item()\n",
        "      total_malicious_samples += malicious_samples.shape[0]\n",
        "\n",
        "  # Calculate and print attack effectiveness\n",
        "  attack_effectiveness = (successful_attacks / total_malicious_samples) * 100 if total_malicious_samples > 0 else 0\n",
        "  print(f\"Mimic attack effectiveness: {attack_effectiveness:.3f}%.\")\n",
        "\n",
        "  return attack_effectiveness  # Added return statement for clarity\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kVA4nOM0YUVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mimicry(ben_x, malicious_samples, model_DNN, trials=30, seed=230, is_report_loss_diff=False):\n",
        "    \"\"\"\n",
        "    Perform a mimicry attack.\n",
        "\n",
        "    Args:\n",
        "    - ben_x (torch.Tensor): Benign samples tensor.\n",
        "    - malicious_samples (torch.Tensor): Malicious samples tensor.\n",
        "    - model_DNN (torch.nn.Module): PyTorch model used for the attack.\n",
        "    - trials (int): Number of trials for the attack.\n",
        "    - seed (int): Random seed for reproducibility.\n",
        "    - is_report_loss_diff (bool): Flag to indicate whether to report attack effectiveness.\n",
        "\n",
        "    Returns:\n",
        "    - adv_x (torch.Tensor): Adversarial examples tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure trials do not exceed the length of ben_x\n",
        "    trials = min(trials, len(ben_x))\n",
        "\n",
        "    # Get the number of malicious samples\n",
        "    n_samples = len(malicious_samples)\n",
        "\n",
        "    if n_samples > 0:\n",
        "        # Expand dimensions for efficient broadcasting\n",
        "        malicious_samples_expanded = malicious_samples.unsqueeze(1).expand(-1, trials, -1)\n",
        "\n",
        "        # Generate random indices for sampling from ben_x\n",
        "        torch.manual_seed(seed)\n",
        "        indices = torch.randperm(len(ben_x), device=ben_x.device)[:trials]\n",
        "        trial_vectors_expanded = ben_x[indices].unsqueeze(0)\n",
        "\n",
        "        # Perform the mimic attack\n",
        "        pertbx = torch.clamp(malicious_samples_expanded + trial_vectors_expanded, min=0., max=1.)\n",
        "\n",
        "        # Compute the loss and check if adversarial examples are successful\n",
        "        loss, done = get_loss(pertbx.view(-1, pertbx.shape[-1]), torch.ones(n_samples * trials, 1, device=ben_x.device), model_DNN)\n",
        "\n",
        "        # Add maximum loss to successful attacks to differentiate\n",
        "        max_v = loss.max()\n",
        "        loss[done] += max_v\n",
        "\n",
        "        # Reshape the loss and done tensors\n",
        "        loss = loss.view(n_samples, trials)\n",
        "        done = done.view(n_samples, trials)\n",
        "\n",
        "        # Report attack effectiveness if required\n",
        "        if is_report_loss_diff:\n",
        "            n_done = torch.any(done, dim=-1).sum()\n",
        "            print(f\"Mimicry*{trials}: Attack effectiveness {n_done / n_samples * 100:.3f}%.\")\n",
        "\n",
        "        # Get the index of the maximum loss for each sample\n",
        "        _, indices = loss.max(dim=-1)\n",
        "        adv_x = pertbx[torch.arange(n_samples), indices]\n",
        "\n",
        "        del pertbx, loss, done, malicious_samples_expanded, trial_vectors_expanded\n",
        "\n",
        "        return adv_x\n",
        "    else:\n",
        "        print(\"No malicious samples found.\")\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pdqmpaw3sIUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PGD_Max(x,y, model, attack_list = ['linf', 'l2', 'l1'],steps_max=5, is_sample = False, varepsilon = 1e-20):\n",
        "    \"\"\"\n",
        "    PGD_Max adversarial attack.\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [samples, features])\n",
        "        y: Ground truth labels tensor (shape: [samples])\n",
        "        model: Neural network model\n",
        "        attack_list: List of norms for attacks (default: ['linf', 'l2', 'l1'])\n",
        "        steps_max: Maximum number of steps (default: 5)\n",
        "        is_sample: Flag to sample randomly from the feasible area (default: False)\n",
        "        vaρεpsilon: Tolerance for stopping condition (default: 1e-20)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial version of input data (tensor)\n",
        "    \"\"\"\n",
        "    batch_size = x.shape[0]\n",
        "    norm_params = {\n",
        "        'l1': {'k': 50, 'step_length': 1.0},\n",
        "        'l2': {'k': 200, 'step_length': 0.05},\n",
        "        'linf': {'k': 500, 'step_length': 0.002}\n",
        "    }\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss, done = get_loss(x,y,model) #shape:[samples],[samples]\n",
        "\n",
        "    pre_loss = loss\n",
        "    n = x.shape[0]\n",
        "    adv_x = x.detach().clone()\n",
        "    stop_flag = torch.zeros(n, dtype=torch.bool) #[samples]\n",
        "\n",
        "    for t in range(steps_max):\n",
        "      num_remaining  = (~stop_flag).sum().item()\n",
        "      print('number of remaining samples : ',num_remaining )\n",
        "      if num_remaining  <= 0:\n",
        "          break\n",
        "\n",
        "      remaining_label = y[~stop_flag]\n",
        "      pertbx = []\n",
        "\n",
        "      for norm in attack_list:\n",
        "          if norm in norm_params:\n",
        "              params = norm_params[norm]\n",
        "              perturbation = pgd(adv_x[~stop_flag], remaining_label, model, norm=norm, is_sample=is_sample, **params)\n",
        "              print(\"the number of added features : \", (perturbation.sum() - adv_x[~stop_flag].sum())/len(adv_x[~stop_flag]))\n",
        "              pertbx.append(perturbation)\n",
        "          else:\n",
        "              raise ValueError(\"Expected 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "\n",
        "      # here pertbx.shape = a list of (number of attacks  ,(num_remaining ,features))\n",
        "      pertbx = torch.vstack(pertbx)\n",
        "      # here pertbx.shape = a tensor (num_remaining *number of attacks samples, features)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        remaining_label_ext = torch.cat([remaining_label] * len(attack_list)) #(labels*number of attacks )\n",
        "        loss, done = get_loss(pertbx, remaining_label_ext,model) #(labels*number of attacks )\n",
        "        loss = loss.reshape(len(attack_list), num_remaining ).permute(1, 0) #(num_remaining ,number of attacks)\n",
        "        done = done.reshape(len(attack_list), num_remaining ).permute(1, 0) #(num_remaining ,number of attacks)\n",
        "\n",
        "        success_flag = torch.any(done, dim=-1) #(num_remaining )\n",
        "        # for a sample, if there is at least one successful attack, we will select the one with maximum loss;\n",
        "        # while if no attacks evade the victim successful, all perturbed examples are reminded for selection\n",
        "\n",
        "        done[~torch.any(done, dim=-1)] = 1 #loss.shape=done.shape=(samples,number of attacks)\n",
        "        loss = (loss * done.to(torch.float)) + torch.min(loss) * (~done).to(torch.float) #(num_remaining ,number of attacks)\n",
        "        pertbx = pertbx.reshape(len(attack_list), num_remaining , x.shape[1]).permute([1, 0, 2])#(num_remaining ,attacks,features)\n",
        "        _, indices = loss.max(dim=-1) # ans:(samples), max loss among attacks which worked, and max loss among all attacks for sample , none of them worked\n",
        "        adv_x[~stop_flag] = pertbx[torch.arange(num_remaining ), indices]\n",
        "        a_loss = loss[torch.arange(num_remaining ), indices]\n",
        "        pre_stop_flag = stop_flag.clone()\n",
        "        stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < varepsilon) | success_flag\n",
        "        pre_loss[~pre_stop_flag] = a_loss\n",
        "\n",
        "    return adv_x"
      ],
      "metadata": {
        "id": "c589J_H1GcGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PGD_Max2(x,y, model, attack_list = ['linf', 'l2', 'l1'],steps_max=5, is_sample = False, varepsilon = 1e-20):\n",
        "    \"\"\"\n",
        "    PGD_Max adversarial attack.\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [samples, features])\n",
        "        y: Ground truth labels tensor (shape: [samples])\n",
        "        model: Neural network model\n",
        "        attack_list: List of norms for attacks (default: ['linf', 'l2', 'l1'])\n",
        "        steps_max: Maximum number of steps (default: 5)\n",
        "        is_sample: Flag to sample randomly from the feasible area (default: False)\n",
        "        vaρεpsilon: Tolerance for stopping condition (default: 1e-20)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial version of input data (tensor)\n",
        "    \"\"\"\n",
        "    batch_size = x.shape[0]\n",
        "    norm_params = {\n",
        "        'l1': {'k': 50, 'step_length': 1.0},\n",
        "        'l2': {'k': 200, 'step_length': 0.05},\n",
        "        'linf': {'k': 500, 'step_length': 0.002}\n",
        "    }\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss, done = get_loss(x,y,model) #shape:[samples],[samples]\n",
        "\n",
        "    pre_loss = loss\n",
        "    n = x.shape[0]\n",
        "    adv_x = x.detach().clone()\n",
        "    stop_flag = torch.zeros(n, dtype=torch.bool) #[samples]\n",
        "\n",
        "    for t in range(steps_max):\n",
        "      num_remaining  = (~stop_flag).sum().item()\n",
        "      print('number of remaining samples : ',num_remaining )\n",
        "      if num_remaining  <= 0:\n",
        "          break\n",
        "\n",
        "      remaining_label = y[~stop_flag]\n",
        "      pertbx = []\n",
        "\n",
        "      for norm in attack_list:\n",
        "          if norm in norm_params:\n",
        "              params = norm_params[norm]\n",
        "              perturbation = pgd(adv_x[~stop_flag], remaining_label, model, norm=norm, is_sample=is_sample, **params)\n",
        "              print(\"the number of added features : \", (perturbation.sum() - adv_x[~stop_flag].sum())/len(adv_x[~stop_flag]))\n",
        "              pertbx.append(perturbation)\n",
        "          else:\n",
        "              raise ValueError(\"Expected 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "\n",
        "      # here pertbx.shape = a list of (number of attacks  ,(num_remaining ,features))\n",
        "      pertbx = torch.vstack(pertbx)\n",
        "      # here pertbx.shape = a tensor (num_remaining *number of attacks samples, features)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        remaining_label_ext = torch.cat([remaining_label] * len(attack_list)) #(labels*number of attacks )\n",
        "        loss, done = get_loss(pertbx, remaining_label_ext,model) #(labels*number of attacks )\n",
        "\n",
        "        # for a sample, if there is at least one successful attack, we will select the one with maximum loss;\n",
        "        # while if no attacks evade the victim successful, all perturbed examples are reminded for selection\n",
        "        max_v = loss.amax()\n",
        "        loss[done] += max_v\n",
        "\n",
        "        loss = loss.reshape(len(attack_list), num_remaining ).permute(1, 0) #(num_remaining ,number of attacks)\n",
        "        done = done.reshape(len(attack_list), num_remaining ).permute(1, 0) #(num_remaining ,number of attacks)\n",
        "\n",
        "        success_flag = torch.any(done, dim=-1) #(num_remaining )\n",
        "\n",
        "        pertbx = pertbx.reshape(len(attack_list), num_remaining , x.shape[1]).permute([1, 0, 2])#(num_remaining ,attacks,features)\n",
        "        _, indices = loss.max(dim=-1) # ans:(samples), max loss among attacks which worked, and max loss among all attacks for sample , none of them worked\n",
        "        adv_x[~stop_flag] = pertbx[torch.arange(num_remaining ), indices]\n",
        "        a_loss = loss[torch.arange(num_remaining ), indices]\n",
        "        pre_stop_flag = stop_flag.clone()\n",
        "        stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < varepsilon) | success_flag\n",
        "        pre_loss[~pre_stop_flag] = a_loss\n",
        "\n",
        "    return adv_x"
      ],
      "metadata": {
        "id": "d2Rw-5_fJU4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_step(x, y, model, norm, k, step_length):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack for stepwise.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :return: The adversarial version of x (tensor)(not rounded)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        grad_data = grad_vars[0].data\n",
        "        gradients = grad_data * (x < 0.5)\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients)\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm)\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "        elif norm == 'l1':\n",
        "            #ignore the gradient of indice which is updated\n",
        "            gradients = gradients * (x_next < 0.5)\n",
        "            val, _ = torch.topk(gradients, 1)\n",
        "            perturbation = torch.sign(gradients >= val.expand_as(gradients))\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            outputs = model(x_next)\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            done = (predicted != y).squeeze()\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    #remove negative pertubations, we cant use OR function because we have values between (0,1) like 0.2 which we want to keep\n",
        "    x_adv = (((x_next - x) >= 0) * x_next) + (((x_next - x) < 0) * x)\n",
        "    return x_adv"
      ],
      "metadata": {
        "id": "GP2Fx9BOd-zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StepwiseMax(\n",
        "    x,\n",
        "    label,\n",
        "    model,\n",
        "    attack_list=[\"linf\", \"l2\", \"l1\"],\n",
        "    step_lengths={\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002},\n",
        "    steps=100,\n",
        "    step_check = 1,\n",
        "    random_start=False,\n",
        "    round_threshold=0.5,\n",
        "    is_attacker=False,\n",
        "    is_score_round = False\n",
        "):\n",
        "  \"\"\"\n",
        "    Stepwise max attack (mixture of pgd-l1, pgd-l2, pgd-linf).\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [batch_size, feature_dim])\n",
        "        label: Ground truth labels tensor (shape: [batch_size])\n",
        "        model: Victim model\n",
        "        attack_list: List of attack norms (default: [\"linf\", \"l2\", \"l1\"])\n",
        "        step_lengths: Dictionary mapping norm to its step length (default: {\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002})\n",
        "        steps: Maximum number of iterations (default: 100)\n",
        "        random_start: Use random starting point (default: False)\n",
        "        round_threshold: Threshold for rounding real scalars (default: 0.5)\n",
        "        is_attacker: Play the role of attacker (default: False)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial examples tensor (same shape as x)\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  step_check = 1\n",
        "  if not is_attacker:\n",
        "      step_checks = [1, 10, 25, 50]\n",
        "      step_check = random.choice(step_checks)\n",
        "\n",
        "  print(f\"Step check: {step_check}\")\n",
        "  mini_steps = [step_check] * (steps // step_check)\n",
        "  if steps % step_check != 0:\n",
        "      mini_steps.append(steps % step_check)\n",
        "\n",
        "  n, red_n = x.shape\n",
        "  adv_x = x.detach().clone()\n",
        "  pert_x_cont = None\n",
        "  prev_done = None\n",
        "  for i, mini_step in enumerate(mini_steps):\n",
        "      with torch.no_grad():\n",
        "          if i == 0 :\n",
        "              adv_x = get_x0(adv_x, initial_rounding_threshold=round_threshold, is_sample=random_start)\n",
        "\n",
        "          _, done = get_loss(adv_x, label, model)\n",
        "      if torch.all(done):\n",
        "          break\n",
        "      if i == 0:\n",
        "          adv_x[~done] = x[~done]  # recompute the perturbation under other penalty factors\n",
        "          prev_done = done\n",
        "      else:\n",
        "          adv_x[~done] = pert_x_cont[~done[~prev_done]]\n",
        "          prev_done = done\n",
        "\n",
        "      num_sample_red = torch.sum(~done).item()\n",
        "      pertbx = []\n",
        "      for norm in attack_list:\n",
        "          step_length = step_lengths.get(norm, step_lengths[\"l1\"])\n",
        "          perturbation = pgd_step(adv_x[~done], label[~done], model, norm, mini_step, step_length)\n",
        "          #print(\"the number of added features(not rounded) \", norm,\": \", perturbation.sum()/len(adv_x[~done]) - adv_x[~done].sum()/len(adv_x[~done]))\n",
        "          #print(\"the number of added features(rounded) \", norm,\" : \",(round_x(perturbation, round_threshold).sum() - round_x(adv_x[~done], round_threshold).sum())/len(adv_x[~done]))\n",
        "          pertbx.append(perturbation)\n",
        "      with torch.no_grad():\n",
        "          pertbx = torch.vstack(pertbx)\n",
        "\n",
        "          n_attacks = len(attack_list)\n",
        "          label_ext = torch.cat([label[~done]] * n_attacks)\n",
        "\n",
        "          if (not is_attacker) and (not is_score_round):\n",
        "              scores, _done = get_loss(pertbx, label_ext,model)\n",
        "          else:\n",
        "              scores, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "          max_v = scores.amax() if scores.amax() > 0 else 0.\n",
        "          scores[_done] += max_v\n",
        "\n",
        "          pertbx = pertbx.reshape(n_attacks, num_sample_red, red_n).permute([1, 0, 2])\n",
        "          scores = scores.reshape(n_attacks, num_sample_red).permute(1, 0)\n",
        "          _2, s_idx = scores.max(dim=-1)\n",
        "          pert_x_cont = pertbx[torch.arange(num_sample_red), s_idx]\n",
        "          adv_x[~done] = pert_x_cont if not is_attacker else round_x(pert_x_cont, round_threshold)\n",
        "\n",
        "  print(i)\n",
        "  if is_attacker:\n",
        "      adv_x = round_x(adv_x, round_threshold)\n",
        "  with torch.no_grad():\n",
        "      _, done = get_loss(adv_x, label, model)\n",
        "      print(f\"step-wise max: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "  return adv_x"
      ],
      "metadata": {
        "id": "1x4bXgnCqSxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StepwiseMax2(\n",
        "    x,\n",
        "    label,\n",
        "    model,\n",
        "    attack_list=[\"linf\", \"l2\", \"l1\"],\n",
        "    step_lengths={\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002},\n",
        "    steps=100,\n",
        "    step_check = 1,\n",
        "    random_start=False,\n",
        "    round_threshold=0.5,\n",
        "    is_attacker=False,\n",
        "    is_score_round = False\n",
        "):\n",
        "  \"\"\"\n",
        "    Stepwise max attack (mixture of pgd-l1, pgd-l2, pgd-linf).\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [batch_size, feature_dim])\n",
        "        label: Ground truth labels tensor (shape: [batch_size])\n",
        "        model: Victim model\n",
        "        attack_list: List of attack norms (default: [\"linf\", \"l2\", \"l1\"])\n",
        "        step_lengths: Dictionary mapping norm to its step length (default: {\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002})\n",
        "        steps: Maximum number of iterations (default: 100)\n",
        "        random_start: Use random starting point (default: False)\n",
        "        round_threshold: Threshold for rounding real scalars (default: 0.5)\n",
        "        is_attacker: Play the role of attacker (default: False)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial examples tensor (same shape as x)\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  step_check = 1\n",
        "  if not is_attacker:\n",
        "      step_checks = [1, 10, 25, 50]\n",
        "      step_check = random.choice(step_checks)\n",
        "\n",
        "  print(f\"Step check: {step_check}\")\n",
        "  mini_steps = [step_check] * (steps // step_check)\n",
        "  if steps % step_check != 0:\n",
        "      mini_steps.append(steps % step_check)\n",
        "  n, red_n = x.shape\n",
        "  adv_x = x.detach().clone()\n",
        "  pert_x_cont = None\n",
        "  prev_done = None\n",
        "  for i, mini_step in enumerate(mini_steps):\n",
        "      with torch.no_grad():\n",
        "          if i == 0 :\n",
        "              adv_x = get_x0(adv_x, initial_rounding_threshold=round_threshold, is_sample=random_start)\n",
        "\n",
        "          if is_attacker:\n",
        "            _, done = get_loss(round_x(adv_x, round_threshold), label, model)\n",
        "          else :\n",
        "            _, done = get_loss(adv_x, label, model)\n",
        "      if torch.all(done):\n",
        "          break\n",
        "      if i == 0:\n",
        "          adv_x[~done] = x[~done]  # recompute the perturbation under other penalty factors\n",
        "          prev_done = done\n",
        "      else:\n",
        "          adv_x[~done] = pert_x_cont[~done[~prev_done]]\n",
        "          prev_done = done\n",
        "\n",
        "      print('len(adv_x[~done]) : ',len(adv_x[~done]))\n",
        "      num_sample_red = torch.sum(~done).item()\n",
        "      pertbx = []\n",
        "      for norm in attack_list:\n",
        "          step_length = step_lengths.get(norm, step_lengths[\"l1\"])\n",
        "          perturbation = pgd_step(adv_x[~done], label[~done], model, norm, mini_step, step_length)\n",
        "          #print(\"the number of added features(not rounded) \", norm,\": \", perturbation.sum()/len(adv_x[~done]) - adv_x[~done].sum()/len(adv_x[~done]))\n",
        "          #print(\"the number of added features(rounded) \", norm,\" : \",(round_x(perturbation, round_threshold).sum() - round_x(adv_x[~done], round_threshold).sum())/len(adv_x[~done]))\n",
        "          pertbx.append(perturbation)\n",
        "      with torch.no_grad():\n",
        "          pertbx = torch.vstack(pertbx)\n",
        "\n",
        "          n_attacks = len(attack_list)\n",
        "          label_ext = torch.cat([label[~done]] * n_attacks)\n",
        "\n",
        "          if (is_score_round):\n",
        "              scores, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "          elif is_attacker:\n",
        "              scores, _ = get_loss(pertbx, label_ext,model)\n",
        "              _, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "          else:\n",
        "              scores, _done = get_loss(pertbx, label_ext,model)\n",
        "\n",
        "          max_v = scores.amax() if scores.amax() > 0 else 0.\n",
        "          scores[_done] += max_v\n",
        "\n",
        "          pertbx = pertbx.reshape(n_attacks, num_sample_red, red_n).permute([1, 0, 2])\n",
        "          scores = scores.reshape(n_attacks, num_sample_red).permute(1, 0)\n",
        "          _2, s_idx = scores.max(dim=-1)\n",
        "          pert_x_cont = pertbx[torch.arange(num_sample_red), s_idx]\n",
        "          adv_x[~done] = pert_x_cont\n",
        "\n",
        "  print(i)\n",
        "  if is_attacker:\n",
        "      adv_x = round_x(adv_x, round_threshold)\n",
        "  with torch.no_grad():\n",
        "      _, done = get_loss(adv_x, label, model)\n",
        "      print(f\"step-wise max: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "  return adv_x"
      ],
      "metadata": {
        "id": "YF48Kp2unI4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_one_step(x, y, model, step_lengths):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack for stepwise.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :return: The adversarial version of x (tensor)(not rounded)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "\n",
        "    # one-step PGD\n",
        "\n",
        "    # Forward pass\n",
        "    x_var = x_next.clone().detach().requires_grad_(True)\n",
        "    y_model = model(x_var)\n",
        "    loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "    # Compute gradient\n",
        "    grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "    grad_data = grad_vars[0].data\n",
        "    gradients = grad_data * (x < 0.5)\n",
        "\n",
        "\n",
        "    # Norms\n",
        "    pertbx = []\n",
        "    # norm = linf\n",
        "    step_length = step_lengths.get(\"linf\", step_lengths[\"l1\"])\n",
        "    perturbation_linf = torch.sign(gradients)\n",
        "    x_next_linf = torch.clamp(x_next + perturbation_linf * step_length, min=0., max=1.)\n",
        "    #remove negative pertubations, we cant use OR function because we have values between (0,1) like 0.2 which we want to keep\n",
        "    x_adv_linf = (((x_next_linf - x) >= 0) * x_next_linf) + (((x_next_linf - x) < 0) * x)\n",
        "    pertbx.append(x_adv_linf)\n",
        "    # norm = l2\n",
        "    step_length = step_lengths.get(\"l2\", step_lengths[\"l1\"])\n",
        "    l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "    perturbation_l2 = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm)\n",
        "    perturbation_l2[torch.isnan(perturbation_l2)] = 0.\n",
        "    perturbation_l2[torch.isinf(perturbation_l2)] = 1.\n",
        "    x_next_l2 = torch.clamp(x_next + perturbation_l2 * step_length, min=0., max=1.)\n",
        "    x_adv_l2 = (((x_next_l2 - x) >= 0) * x_next_l2) + (((x_next_l2 - x) < 0) * x)\n",
        "    pertbx.append(x_adv_l2)\n",
        "    # norm = l1\n",
        "    step_length = step_lengths.get(\"l1\", step_lengths[\"l1\"])\n",
        "    #ignore the gradient of indice which is updated\n",
        "    gradients = gradients * (x_next < 0.5)\n",
        "    val, _ = torch.topk(gradients, 1)\n",
        "    perturbation_l1 = torch.sign(gradients >= val.expand_as(gradients))\n",
        "    x_next_l1 = torch.clamp(x_next + perturbation_l1 * step_length, min=0., max=1.)\n",
        "    x_adv_l1 = (((x_next_l1 - x) >= 0) * x_next_l1) + (((x_next_l1 - x) < 0) * x)\n",
        "    pertbx.append(x_adv_l1)\n",
        "\n",
        "    return pertbx"
      ],
      "metadata": {
        "id": "De3vBz1IHpY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uh6L653jYA1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StepwiseMax_onestep(\n",
        "    x,\n",
        "    label,\n",
        "    model,\n",
        "    attack_list=[\"linf\", \"l2\", \"l1\"],\n",
        "    step_lengths={\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002},\n",
        "    steps=100,\n",
        "    random_start=False,\n",
        "    round_threshold=0.5,\n",
        "):\n",
        "  \"\"\"\n",
        "    Stepwise max attack (mixture of pgd-l1, pgd-l2, pgd-linf).\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [batch_size, feature_dim])\n",
        "        label: Ground truth labels tensor (shape: [batch_size])\n",
        "        model: Victim model\n",
        "        attack_list: List of attack norms (default: [\"linf\", \"l2\", \"l1\"])\n",
        "        step_lengths: Dictionary mapping norm to its step length (default: {\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002})\n",
        "        steps: Maximum number of iterations (default: 100)\n",
        "        random_start: Use random starting point (default: False)\n",
        "        round_threshold: Threshold for rounding real scalars (default: 0.5)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial examples tensor (same shape as x)\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  n, red_n = x.shape\n",
        "  adv_x = x.detach().clone()\n",
        "  pert_x_cont = None\n",
        "  prev_done = None\n",
        "  for step in range(steps):\n",
        "      with torch.no_grad():\n",
        "          if step == 0 :\n",
        "              adv_x = get_x0(adv_x, initial_rounding_threshold=round_threshold, is_sample=random_start)\n",
        "\n",
        "          _, done = get_loss(round_x(adv_x, round_threshold), label, model)\n",
        "\n",
        "      if torch.all(done):\n",
        "          break\n",
        "      if step == 0:\n",
        "          adv_x[~done] = x[~done]  # recompute the perturbation under other penalty factors\n",
        "          prev_done = done\n",
        "      else:\n",
        "          adv_x[~done] = pert_x_cont[~done[~prev_done]]\n",
        "          prev_done = done\n",
        "\n",
        "      print('len(adv_x[~done]) : ',len(adv_x[~done]))\n",
        "      num_sample_red = torch.sum(~done).item()\n",
        "\n",
        "      pertbx = pgd_one_step(adv_x[~done], label[~done], model, step_lengths)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          pertbx = torch.vstack(pertbx)\n",
        "\n",
        "          n_attacks = len(attack_list)\n",
        "          label_ext = torch.cat([label[~done]] * n_attacks)\n",
        "\n",
        "          scores, _ = get_loss(pertbx, label_ext,model)\n",
        "          _, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "\n",
        "          max_v = scores.amax() if scores.amax() > 0 else 0.\n",
        "          scores[_done] += max_v\n",
        "\n",
        "          pertbx = pertbx.reshape(n_attacks, num_sample_red, red_n).permute([1, 0, 2])\n",
        "          scores = scores.reshape(n_attacks, num_sample_red).permute(1, 0)\n",
        "          _2, s_idx = scores.max(dim=-1)\n",
        "          pert_x_cont = pertbx[torch.arange(num_sample_red), s_idx]\n",
        "          adv_x[~done] = pert_x_cont\n",
        "\n",
        "  print(step)\n",
        "  adv_x = round_x(adv_x, round_threshold)\n",
        "  with torch.no_grad():\n",
        "      _, done = get_loss(adv_x, label, model)\n",
        "      print(f\"step-wise max: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "  return adv_x"
      ],
      "metadata": {
        "id": "SSiNK4V_PaFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def around_x(x, std_deviation=0.1, random_start=False):\n",
        "    \"\"\"\n",
        "    Helper function to randomly initialize the inner maximizer algorithm.\n",
        "    Randomizes the input tensor while preserving its functionality.\n",
        "    :param x: input tensor\n",
        "    :param std_deviation: std_deviation for domain\n",
        "    :param random_start: flag to sample randomly from feasible area\n",
        "    :return: randomly sampled feasible version of x\n",
        "    \"\"\"\n",
        "    if random_start:\n",
        "\n",
        "        # Generate random tensor from a Gaussian distribution centered around zero\n",
        "        random_tensor = abs(torch.randn(x.size()))\n",
        "\n",
        "        # Scale the values to control the spread of the distribution\n",
        "        random_tensor *= std_deviation\n",
        "\n",
        "        return torch.clamp(x + random_tensor, min=0., max=1.)\n",
        "    else:\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Bfshb8AgqPtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_one_step2(x, y, model, step_lengths,x_initial):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack for stepwise.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :return: The adversarial version of x (tensor)(not rounded)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "\n",
        "    # one-step PGD\n",
        "\n",
        "    # Forward pass\n",
        "    x_var = x_next.clone().detach().requires_grad_(True)\n",
        "    y_model = model(x_var)\n",
        "    loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "    # Compute gradient\n",
        "    grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "    grad_data = grad_vars[0].data\n",
        "    gradients = grad_data * (x_initial < 0.5)\n",
        "\n",
        "\n",
        "    # Norms\n",
        "    pertbx = []\n",
        "    # norm = linf\n",
        "    step_length = step_lengths.get(\"linf\", step_lengths[\"l1\"])\n",
        "    perturbation_linf = torch.sign(gradients)\n",
        "    x_next_linf = torch.clamp(x_next + perturbation_linf * step_length, min=0., max=1.)\n",
        "    #remove negative pertubations, we cant use OR function because we have values between (0,1) like 0.2 which we want to keep\n",
        "    x_adv_linf = (((x_next_linf - x_initial) >= 0) * x_next_linf) + (((x_next_linf - x_initial) < 0) * x_initial)\n",
        "    pertbx.append(x_adv_linf)\n",
        "    # norm = l2\n",
        "    step_length = step_lengths.get(\"l2\", step_lengths[\"l1\"])\n",
        "    l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "    perturbation_l2 = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm)\n",
        "    perturbation_l2[torch.isnan(perturbation_l2)] = 0.\n",
        "    perturbation_l2[torch.isinf(perturbation_l2)] = 1.\n",
        "    x_next_l2 = torch.clamp(x_next + perturbation_l2 * step_length, min=0., max=1.)\n",
        "    x_adv_l2 = (((x_next_l2 - x_initial) >= 0) * x_next_l2) + (((x_next_l2 - x_initial) < 0) * x_initial)\n",
        "    pertbx.append(x_adv_l2)\n",
        "    # norm = l1\n",
        "    step_length = step_lengths.get(\"l1\", step_lengths[\"l1\"])\n",
        "    #ignore the gradient of indice which is updated\n",
        "    gradients_l1 = gradients * (x_next < 0.5)\n",
        "    val, _ = torch.topk(gradients_l1, 1)\n",
        "\n",
        "    perturbation_l1 = torch.sign(gradients >= val.expand_as(gradients)) * (val > 1e-10)\n",
        "\n",
        "    x_next_l1 = torch.clamp(x_next + perturbation_l1 * step_length, min=0., max=1.)\n",
        "    x_adv_l1 = (((x_next_l1 - x_initial) >= 0) * x_next_l1) + (((x_next_l1 - x_initial) < 0) * x_initial)\n",
        "    pertbx.append(x_adv_l1)\n",
        "\n",
        "    return pertbx"
      ],
      "metadata": {
        "id": "VIny0BjsYBFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StepwiseMax_onestep2(\n",
        "    x,\n",
        "    label,\n",
        "    model,\n",
        "    attack_list=[\"linf\", \"l2\", \"l1\"],\n",
        "    step_lengths={\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002},\n",
        "    steps=100,\n",
        "    random_start=False,\n",
        "    round_threshold=0.5,\n",
        "):\n",
        "  \"\"\"\n",
        "    Stepwise max attack (mixture of pgd-l1, pgd-l2, pgd-linf).\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [batch_size, feature_dim])\n",
        "        label: Ground truth labels tensor (shape: [batch_size])\n",
        "        model: Victim model\n",
        "        attack_list: List of attack norms (default: [\"linf\", \"l2\", \"l1\"])\n",
        "        step_lengths: Dictionary mapping norm to its step length (default: {\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002})\n",
        "        steps: Maximum number of iterations (default: 100)\n",
        "        random_start: Use random starting point (default: False)\n",
        "        round_threshold: Threshold for rounding real scalars (default: 0.5)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial examples tensor (same shape as x)\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  n, red_n = x.shape\n",
        "  adv_x = x.detach().clone()\n",
        "  pert_x_cont = None\n",
        "  prev_done = None\n",
        "  for step in range(steps):\n",
        "      with torch.no_grad():\n",
        "          if step == 0 :\n",
        "              adv_x = around_x(adv_x, std_deviation=0.1, random_start=random_start)\n",
        "\n",
        "          _, done = get_loss(round_x(adv_x, round_threshold), label, model)\n",
        "\n",
        "      if torch.all(done):\n",
        "          break\n",
        "      if step == 0:\n",
        "          adv_x[~done] = x[~done]  # recompute the perturbation under other penalty factors\n",
        "          prev_done = done\n",
        "      else:\n",
        "          adv_x[~done] = pert_x_cont[~done[~prev_done]]\n",
        "          prev_done = done\n",
        "\n",
        "      #print('remaining samples : ',len(adv_x[~done]))\n",
        "      num_sample_red = torch.sum(~done).item()\n",
        "\n",
        "      pertbx = pgd_one_step2(adv_x[~done], label[~done], model, step_lengths,x[~done])\n",
        "\n",
        "      with torch.no_grad():\n",
        "          pertbx = torch.vstack(pertbx)\n",
        "\n",
        "          n_attacks = len(attack_list)\n",
        "          label_ext = torch.cat([label[~done]] * n_attacks)\n",
        "\n",
        "          scores, _ = get_loss(pertbx, label_ext,model)\n",
        "          _, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "\n",
        "          max_v = scores.amax() if scores.amax() > 0 else 0.\n",
        "          scores[_done] += max_v\n",
        "\n",
        "          pertbx = pertbx.reshape(n_attacks, num_sample_red, red_n).permute([1, 0, 2])\n",
        "          scores = scores.reshape(n_attacks, num_sample_red).permute(1, 0)\n",
        "          _2, s_idx = scores.max(dim=-1)\n",
        "          #print('best attack : ',s_idx)\n",
        "          pert_x_cont = pertbx[torch.arange(num_sample_red), s_idx]\n",
        "          adv_x[~done] = pert_x_cont\n",
        "\n",
        "  #print(step)\n",
        "  adv_x = round_x(adv_x, round_threshold)\n",
        "  with torch.no_grad():\n",
        "      _, done = get_loss(adv_x, label, model)\n",
        "      print(f\"step-wise max: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "  return adv_x"
      ],
      "metadata": {
        "id": "zRBrmMyWYHn4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}